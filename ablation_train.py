
# enhanced_semantic_vqgan_resume_fixed6.py - ä¿®å¤è§£ç å™¨è¾“å…¥é—®é¢˜

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import os
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import matplotlib.pyplot as plt
from tqdm import tqdm
import wandb
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import lpips
from torchvision.models import vgg16
import glob
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from collections import Counter
import gc

# å¯¼å…¥åŸæœ‰çš„æ‰€æœ‰ç±»
from train import (
    FixedSemanticCodebook,
    ImprovedSemanticEncoder,
    PatchDiscriminator,
    COCOImageDataset,
    CaptionGenerator,
    save_samples,
    calculate_psnr,
    calculate_ssim
)


# ================== æ–°çš„çµæ´»è§£ç å™¨ï¼ˆåŠ¨æ€ä¸Šé‡‡æ ·ç‰ˆæœ¬ï¼‰ ==================
class FlexibleSemanticDecoder(nn.Module):
    """
    æ”¯æŒçµæ´»è¾“å…¥ç©ºé—´å°ºå¯¸çš„è§£ç å™¨
    å¯æ ¹æ®ç¼–ç å™¨è¾“å‡ºç©ºé—´å°ºå¯¸å’Œç›®æ ‡å›¾åƒå°ºå¯¸åŠ¨æ€æ„å»ºä¸Šé‡‡æ ·å±‚
    """

    def __init__(self, in_channels: int, start_spatial_size: int,
                 base_channels: int = 64, target_size: int = 128):
        super().__init__()

        layers = []
        current_size = start_spatial_size
        current_channels = in_channels
        channels = base_channels

        # å¾ªç¯ä¸Šé‡‡æ ·ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡å°ºå¯¸
        while current_size < target_size:
            layers.append(
                nn.ConvTranspose2d(current_channels, channels, kernel_size=4, stride=2, padding=1)
            )
            layers.append(nn.BatchNorm2d(channels))
            layers.append(nn.ReLU(True))

            current_size *= 2
            current_channels = channels
            channels //= 2
            if channels < 8:
                channels = 8

        # æœ€ç»ˆå·ç§¯åˆ° 3 é€šé“
        layers.append(nn.Conv2d(current_channels, 3, kernel_size=3, stride=1, padding=1))
        layers.append(nn.Tanh())

        self.layers = nn.Sequential(*layers)
        print(f"[Decoder] Upsample from {start_spatial_size} -> {target_size} in {len(layers)//3} steps")

    def forward(self, x):
        return self.layers(x)


# ================== ä¿®æ”¹åçš„ CustomizableSemanticVQGAN éƒ¨åˆ† ==================
class CustomizableSemanticVQGAN(nn.Module):
    def __init__(self, config: dict):
        super().__init__()
        
        self.enabled_codebooks = config['model'].get('enabled_codebooks', 
                                                     ['global', 'object', 'color', 'spatial', 'texture'])
        print(f"\nğŸ”§ Enabled codebooks: {self.enabled_codebooks}")
        
        self.total_codebook_dim = sum(
            config['model']['codebooks'][name]['embedding_dim'] 
            for name in self.enabled_codebooks
        )
        
        # åˆ›å»ºç¼–ç å™¨
        self.encoder = ImprovedSemanticEncoder(
            in_channels=3,
            base_channels=config['model']['encoder']['base_channels']
        )
        
        # è·å–ç¼–ç å™¨è¾“å‡ºç»´åº¦å’Œç©ºé—´å°ºå¯¸
        with torch.no_grad():
            dummy_input = torch.zeros(1, 3, config['data']['image_size'], config['data']['image_size'])
            encoder_output = self.encoder(dummy_input)
            if isinstance(encoder_output, dict):
                if 'features' in encoder_output:
                    h = encoder_output['features']
                elif 'h' in encoder_output:
                    h = encoder_output['h']
                elif 'output' in encoder_output:
                    h = encoder_output['output']
                else:
                    for value in encoder_output.values():
                        if isinstance(value, torch.Tensor) and value.dim() == 4:
                            h = value
                            break
            else:
                h = encoder_output
            
            self.encoder_out_dim = h.shape[1]
            self.encoder_spatial_size = h.shape[2]
            print(f"Detected encoder output: {self.encoder_out_dim} channels, "
                  f"{self.encoder_spatial_size}x{self.encoder_spatial_size} spatial")
        
        # æŠ•å½±å±‚
        if self.encoder_out_dim != self.total_codebook_dim:
            self.encoder_proj = nn.Conv2d(self.encoder_out_dim, self.total_codebook_dim, kernel_size=1)
            print(f"Added encoder projection: {self.encoder_out_dim} -> {self.total_codebook_dim}")
        else:
            self.encoder_proj = nn.Identity()
        
        # â­ åŠ¨æ€è§£ç å™¨
        self.decoder = FlexibleSemanticDecoder(
            in_channels=self.total_codebook_dim,
            start_spatial_size=self.encoder_spatial_size,
            base_channels=config['model']['decoder']['base_channels'],
            target_size=config['data']['image_size']
        )
        print(f"Created decoder with input channels: {self.total_codebook_dim}")
        
        # åç»­ codebook åˆ›å»ºç­‰ä¿æŒåŸæ · ...
        
        # åªåˆ›å»ºå¯ç”¨çš„ç æœ¬
        self.codebooks = nn.ModuleDict()
        for name in self.enabled_codebooks:
            cb_config = config['model']['codebooks'][name]
            self.codebooks[name] = FixedSemanticCodebook(
                codebook_path=cb_config['path'],
                embedding_dim=cb_config['embedding_dim'],
                commitment_cost=cb_config.get('commitment_cost', 0.25),
                decay=cb_config.get('decay', 0.99),
                epsilon=cb_config.get('epsilon', 1e-5)
            )
        
        # GANç›¸å…³
        self.use_gan = config['training'].get('use_gan', False)
        if self.use_gan:
            self.discriminator = PatchDiscriminator(
                in_channels=3,
                base_channels=config['model'].get('discriminator_channels', 64),
                n_layers=3
            )
        
        # å­˜å‚¨ç æœ¬ç»´åº¦ä¿¡æ¯
        self.codebook_dims = {
            name: config['model']['codebooks'][name]['embedding_dim']
            for name in self.enabled_codebooks
        }
        
        # è®¡ç®—åˆ‡åˆ†ç‚¹
        self.split_points = []
        current_dim = 0
        for name in self.enabled_codebooks:
            self.split_points.append(current_dim)
            current_dim += self.codebook_dims[name]
        self.split_points.append(current_dim)
    
    def encode(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ç¼–ç å›¾åƒåˆ°å¤šä¸ªè¯­ä¹‰ç æœ¬ç©ºé—´"""
        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        encoder_output = self.encoder(x)
        
        # å¤„ç†å­—å…¸æˆ–å¼ é‡è¾“å‡º
        if isinstance(encoder_output, dict):
            # å¦‚æœæ˜¯å­—å…¸ï¼Œæ‰¾åˆ°ä¸»è¦çš„ç‰¹å¾å¼ é‡
            if 'features' in encoder_output:
                h = encoder_output['features']
            elif 'h' in encoder_output:
                h = encoder_output['h']
            elif 'output' in encoder_output:
                h = encoder_output['output']
            else:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ª4Då¼ é‡
                for value in encoder_output.values():
                    if isinstance(value, torch.Tensor) and value.dim() == 4:
                        h = value
                        break
        else:
            h = encoder_output
        
        # åº”ç”¨æŠ•å½±å±‚
        h = self.encoder_proj(h)
        
        B, C, H, W = h.shape
        
        # æŒ‰ç…§å¯ç”¨çš„ç æœ¬åˆ‡åˆ†ç‰¹å¾
        features = {}
        for i, name in enumerate(self.enabled_codebooks):
            start_dim = self.split_points[i]
            end_dim = self.split_points[i + 1]
            features[name] = h[:, start_dim:end_dim, :, :]
        
        return features
    
    def decode(self, quantized_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """ä»é‡åŒ–ç‰¹å¾è§£ç å›¾åƒ"""
        # æŒ‰ç…§å¯ç”¨çš„ç æœ¬é¡ºåºæ‹¼æ¥ç‰¹å¾
        features_list = []
        for name in self.enabled_codebooks:
            features_list.append(quantized_features[name])
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        combined_features = torch.cat(features_list, dim=1)
        
        # ä½¿ç”¨çµæ´»è§£ç å™¨è§£ç 
        reconstructed = self.decoder(combined_features)
        return reconstructed
    
    def forward(self, x: torch.Tensor, return_tokens: bool = False) -> Tuple[torch.Tensor, dict]:
        """å‰å‘ä¼ æ’­"""
        # ç¼–ç 
        encoded_features = self.encode(x)
        
        # é‡åŒ–æ¯ä¸ªç æœ¬
        quantized_features = {}
        vq_losses = {}
        vq_info = {}
        
        for name in self.enabled_codebooks:
            # è°ƒç”¨ç æœ¬è¿›è¡Œé‡åŒ–
            result = self.codebooks[name](encoded_features[name])
            
            # è§£æè¿”å›å€¼ - FixedSemanticCodebook è¿”å› (quantized, loss_dict)
            if isinstance(result, tuple) and len(result) == 2:
                quantized, loss_dict = result
                quantized_features[name] = quantized
                vq_losses[name] = loss_dict['loss']
                vq_info[name] = {
                    'perplexity': loss_dict.get('perplexity', torch.tensor(0.0)),
                    'usage': loss_dict.get('usage', 0.0),
                    'encoding_indices': loss_dict.get('encoding_indices', None)
                }
                if return_tokens and 'encoding_indices' in loss_dict:
                    vq_info[name]['tokens'] = loss_dict['encoding_indices'].cpu().numpy().tolist()
            else:
                # å¦‚æœè¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼ŒæŠ›å‡ºé”™è¯¯
                raise ValueError(f"Unexpected return format from codebook {name}: {type(result)}")
        
        # è§£ç 
        reconstructed = self.decode(quantized_features)
        
        # æ•´ç†è¿”å›ä¿¡æ¯
        info = {
            'vq_losses': vq_losses,
            'vq_info': vq_info
        }
        
        return reconstructed, info

# ================== ä¿®æ”¹åçš„Captionç”Ÿæˆå™¨ ==================
class FlexibleCaptionGenerator:
    """æ”¯æŒçµæ´»ç æœ¬é…ç½®çš„Captionç”Ÿæˆå™¨"""
    
    def __init__(self, enabled_codebooks: List[str]):
        self.enabled_codebooks = enabled_codebooks
        self.templates = {
            'global': [
                "This image shows {}",
                "A photograph of {}",
                "An image depicting {}",
                "This is {}"
            ],
            'object': [
                "featuring {} prominently",
                "with {} visible",
                "containing {}",
                "showing {}"
            ],
            'color': [
                "in {} tones",
                "with {} colors",
                "displaying {} hues",
                "colored in {}"
            ],
            'spatial': [
                "arranged {}",
                "positioned {}",
                "with {} composition",
                "laid out {}"
            ],
            'texture': [
                "with {} texture",
                "having {} surface",
                "showing {} patterns",
                "with {} finish"
            ]
        }
    
    def tokens_to_caption(self, tokens_dict: Dict[str, List[int]]) -> str:
        """å°†tokensè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°"""
        caption_parts = []
        
        # åªå¤„ç†å¯ç”¨çš„ç æœ¬
        for codebook_name in self.enabled_codebooks:
            if codebook_name in tokens_dict and tokens_dict[codebook_name]:
                tokens = tokens_dict[codebook_name][0] if isinstance(tokens_dict[codebook_name][0], list) else [tokens_dict[codebook_name][0]]
                
                # åŸºäºtokenç”Ÿæˆæè¿°
                if codebook_name == 'global':
                    descriptions = ["scene " + str(t % 10) for t in tokens[:3]]
                elif codebook_name == 'object':
                    object_types = ["person", "car", "animal", "building", "tree", "furniture", "food", "device"]
                    descriptions = [object_types[t % len(object_types)] for t in tokens[:2]]
                elif codebook_name == 'color':
                    color_types = ["warm", "cool", "vibrant", "muted", "monochrome", "colorful"]
                    descriptions = [color_types[t % len(color_types)] for t in tokens[:1]]
                elif codebook_name == 'spatial':
                    spatial_types = ["centrally", "symmetrically", "diagonally", "scattered", "grouped"]
                    descriptions = [spatial_types[t % len(spatial_types)] for t in tokens[:1]]
                elif codebook_name == 'texture':
                    texture_types = ["smooth", "rough", "detailed", "simple", "complex"]
                    descriptions = [texture_types[t % len(texture_types)] for t in tokens[:1]]
                
                # é€‰æ‹©æ¨¡æ¿å¹¶ç”Ÿæˆæè¿°
                if codebook_name in self.templates and descriptions:
                    template = np.random.choice(self.templates[codebook_name])
                    description = " and ".join(descriptions)
                    caption_parts.append(template.format(description))
        
        # ç»„åˆæ‰€æœ‰éƒ¨åˆ†
        if caption_parts:
            # ç¡®ä¿globalæè¿°åœ¨å‰ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'global' in self.enabled_codebooks and any('This image' in part or 'A photograph' in part for part in caption_parts):
                main_parts = [p for p in caption_parts if 'This image' in p or 'A photograph' in p]
                other_parts = [p for p in caption_parts if p not in main_parts]
                caption = main_parts[0] + ", " + ", ".join(other_parts) + "."
            else:
                caption = ", ".join(caption_parts) + "."
            
            return caption
        else:
            return "An image."

# ================== å…¶ä½™å‡½æ•°ä¿æŒä¸å˜ ==================
def load_fixed_test_images(image_paths: List[str], transform, device):
    """åŠ è½½å›ºå®šçš„æµ‹è¯•å›¾åƒ"""
    test_images = []
    valid_paths = []
    
    for path in image_paths:
        if os.path.exists(path):
            try:
                img = Image.open(path).convert('RGB')
                img_tensor = transform(img).unsqueeze(0).to(device)
                test_images.append(img_tensor)
                valid_paths.append(path)
                print(f"âœ… Loaded test image: {os.path.basename(path)}")
            except Exception as e:
                print(f"âŒ Failed to load {path}: {e}")
        else:
            print(f"âŒ Test image not found: {path}")
    
    if test_images:
        test_images = torch.cat(test_images, dim=0)
        print(f"\nğŸ“¸ Loaded {len(valid_paths)} fixed test images for monitoring")
    else:
        test_images = None
        print("\nâš ï¸  No valid test images loaded")
    
    return test_images, valid_paths

def train_customizable_semantic_vqgan(config: dict, resume_from: Optional[str] = None):
    """æ”¯æŒè‡ªå®šä¹‰ç æœ¬é€‰æ‹©çš„è®­ç»ƒå‡½æ•°"""
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['output_dir'], exist_ok=True)
    
    # è·å–å¯ç”¨çš„ç æœ¬
    enabled_codebooks = config['model'].get('enabled_codebooks', 
                                           ['global', 'object', 'color', 'spatial', 'texture'])
    
    # Captionç”Ÿæˆå™¨ï¼ˆä½¿ç”¨çµæ´»ç‰ˆæœ¬ï¼‰
    caption_generator = FlexibleCaptionGenerator(enabled_codebooks)
    
    # æ•°æ®åŠ è½½
    transform = transforms.Compose([
        transforms.Resize((config['data']['image_size'], config['data']['image_size'])),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # åŠ è½½å›ºå®šçš„æµ‹è¯•å›¾åƒ
    fixed_test_images = None
    fixed_test_paths = []
    if 'fixed_test_images' in config['data'] and config['data']['fixed_test_images']:
        fixed_test_images, fixed_test_paths = load_fixed_test_images(
            config['data']['fixed_test_images'], 
            transform, 
            device
        )
    
    dataset = COCOImageDataset(
        root_dir=config['data']['train_dir'],
        transform=transform,
        max_samples=config['data'].get('max_samples')
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=config['training']['batch_size'],
        shuffle=True,
        num_workers=config['data']['num_workers'],
        pin_memory=True,
        drop_last=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = CustomizableSemanticVQGAN(config).to(device)
    
    # æ‰“å°æ¨¡å‹å‚æ•°é‡å’Œå„ç æœ¬ç»´åº¦
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total model parameters: {total_params:,}")
    print("\nEnabled codebook dimensions:")
    for name in enabled_codebooks:
        dim = config['model']['codebooks'][name].get('embedding_dim', 256)
        print(f"  {name}: {dim}")
    print(f"Total codebook dimension: {model.total_codebook_dim}")
    print(f"Encoder output dimension: {model.encoder_out_dim}")
    
    # ä¼˜åŒ–å™¨
    g_params = list(model.encoder.parameters()) + list(model.decoder.parameters()) + \
               list(model.codebooks.parameters())
    
    # åªåœ¨éœ€è¦æŠ•å½±å±‚æ—¶æ·»åŠ å…¶å‚æ•°
    if not isinstance(model.encoder_proj, nn.Identity):
        g_params.extend(list(model.encoder_proj.parameters()))
    
    g_optimizer = torch.optim.AdamW(g_params, lr=config['training']['learning_rate'], betas=(0.5, 0.999))
    
    d_optimizer = None
    if model.use_gan:
        d_optimizer = torch.optim.AdamW(
            model.discriminator.parameters(),
            lr=config['training']['learning_rate'],
            betas=(0.5, 0.999)
        )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    g_scheduler = CosineAnnealingWarmRestarts(g_optimizer, T_0=len(dataloader), T_mult=2)
    
    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    start_epoch = 0
    global_step = 0
    best_loss = float('inf')
    
    # å¦‚æœæŒ‡å®šäº†æ¢å¤çš„checkpoint
    if resume_from and os.path.exists(resume_from):
        print(f"\nğŸ”„ Resuming training from: {resume_from}")
        checkpoint = torch.load(resume_from, map_location=device)
        
        # æ£€æŸ¥ç æœ¬é…ç½®æ˜¯å¦åŒ¹é…
        if 'config' in checkpoint:
            saved_codebooks = checkpoint['config']['model'].get('enabled_codebooks', 
                                                               ['global', 'object', 'color', 'spatial', 'texture'])
            if set(saved_codebooks) != set(enabled_codebooks):
                print(f"âš ï¸  WARNING: Codebook configuration mismatch!")
                print(f"   Saved: {saved_codebooks}")
                print(f"   Current: {enabled_codebooks}")
                print("   This may cause loading issues or unexpected behavior.")
        
        # æ™ºèƒ½åŠ è½½æ¨¡å‹çŠ¶æ€
        model_state = checkpoint['model_state_dict']
        current_state = model.state_dict()
        
        matched_keys = []
        missing_keys = []
        
        for key in current_state.keys():
            if key in model_state and current_state[key].shape == model_state[key].shape:
                matched_keys.append(key)
            else:
                missing_keys.append(key)
        
        filtered_state = {k: v for k, v in model_state.items() if k in matched_keys}
        model.load_state_dict(filtered_state, strict=False)
        
        print(f"âœ… Loaded {len(matched_keys)} matching keys")
        
        if missing_keys:
            print(f"âš ï¸  Missing keys: {len(missing_keys)}")
            # æ‰“å°å…·ä½“ç¼ºå¤±çš„ç æœ¬
            missing_codebooks = set()
            for key in missing_keys:
                if 'codebooks.' in key:
                    codebook_name = key.split('.')[1]
                    missing_codebooks.add(codebook_name)
            if missing_codebooks:
                print(f"   Missing codebooks: {missing_codebooks}")
        
        # æ¢å¤å…¶ä»–çŠ¶æ€
        if 'optimizer_state_dict' in checkpoint:
            try:
                g_optimizer.load_state_dict(checkpoint['optimizer_state_dict']['g_optimizer'])
                print("âœ… Generator optimizer state loaded")
            except:
                print("âš ï¸  Using fresh generator optimizer")
        
        if 'epoch' in checkpoint:
            start_epoch = checkpoint['epoch'] + 1
        if 'global_step' in checkpoint:
            global_step = checkpoint.get('global_step', 0)
        if 'best_loss' in checkpoint:
            best_loss = checkpoint.get('best_loss', float('inf'))
        
        print(f"\nğŸ“Š Resuming from epoch {start_epoch}, global step {global_step}")
        
        del checkpoint
        gc.collect()
        torch.cuda.empty_cache()
    
    # æŸå¤±å‡½æ•°
    perceptual_loss_fn = lpips.LPIPS(net='vgg').to(device)
    perceptual_loss_fn.eval()
    for param in perceptual_loss_fn.parameters():
        param.requires_grad = False
    
    # è®­ç»ƒå¾ªç¯
    caption_print_interval = config['training'].get('caption_print_interval', 200)
    
    print(f"\nğŸš€ Starting training from epoch {start_epoch}")
    print(f"   GAN training: {'Enabled' if model.use_gan else 'Disabled'}")
    print(f"   Active codebooks: {enabled_codebooks}")
    
    for epoch in range(start_epoch, config['training']['num_epochs']):
        model.train()
        epoch_losses = []
        
        with tqdm(dataloader, desc=f"Epoch {epoch+1}/{config['training']['num_epochs']}") as pbar:
            for batch_idx, (images, captions) in enumerate(pbar):
                images = images.to(device)
                
                # å†³å®šæ˜¯å¦éœ€è¦è¿”å›tokens
                return_tokens = (global_step % caption_print_interval == 0)
                
                # GANè®­ç»ƒæ ‡å¿—
                train_gan = model.use_gan and d_optimizer and global_step > config['training'].get('gan_start_step', 5000)
                
                # === ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒåˆ¤åˆ«å™¨ï¼ˆå¦‚æœå¯ç”¨GANï¼‰ ===
                if train_gan and global_step % config['training']['d_steps_per_g'] == 0:
                    with torch.no_grad():
                        fake_images, _ = model(images, return_tokens=False)
                        fake_images = fake_images.detach()
                    
                    d_optimizer.zero_grad()
                    
                    real_pred = model.discriminator(images)
                    fake_pred = model.discriminator(fake_images)
                    
                    d_loss = F.relu(1.0 - real_pred).mean() + F.relu(1.0 + fake_pred).mean()
                    d_loss.backward()
                    d_optimizer.step()
                
                # === ç¬¬äºŒæ­¥ï¼šè®­ç»ƒç”Ÿæˆå™¨ ===
                reconstructed, info = model(images, return_tokens=return_tokens)
                
                # 1. é‡å»ºæŸå¤±
                recon_loss = F.mse_loss(reconstructed, images) * config['training']['loss_weights']['reconstruction']
                
                # 2. æ„ŸçŸ¥æŸå¤±
                with torch.no_grad():
                    perceptual_loss_value = perceptual_loss_fn(reconstructed, images).mean().item()
                perceptual_loss = perceptual_loss_value * config['training']['loss_weights']['perceptual']
                
                # 3. VQæŸå¤±ï¼ˆåªè®¡ç®—å¯ç”¨çš„ç æœ¬ï¼‰
                vq_loss = sum(info['vq_losses'].values()) * config['training']['loss_weights']['vq']
                
                # 4. GANæŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                gan_loss_value = 0.0
                if train_gan:
                    fake_pred_g = model.discriminator(reconstructed)
                    gan_loss = -fake_pred_g.mean() * config['training']['loss_weights']['gan']
                    gan_loss_value = gan_loss.item()
                    g_loss = recon_loss + vq_loss + perceptual_loss + gan_loss
                else:
                    g_loss = recon_loss + vq_loss + perceptual_loss
                
                # ç”Ÿæˆå™¨ä¼˜åŒ–
                g_optimizer.zero_grad(set_to_none=True)
                g_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(g_params, config['training']['gradient_clip'])
                
                g_optimizer.step()
                g_scheduler.step()
                
                # è®°å½•æŸå¤±
                total_loss = g_loss.item()
                epoch_losses.append(total_loss)
                
                # æ›´æ–°è¿›åº¦æ¡
                with torch.no_grad():
                    pbar.set_postfix({
                        'loss': f"{total_loss:.2f}",
                        'recon': f"{F.mse_loss(reconstructed, images).item():.3f}",
                        'gan': f"{gan_loss_value:.3f}",
                        'lr': f"{g_optimizer.param_groups[0]['lr']:.2e}"
                    })
                
                # æ‰“å°captionå¯¹æ¯”ï¼ˆåªä½¿ç”¨å¯ç”¨çš„ç æœ¬ï¼‰
                if return_tokens and any(info['vq_info'][cb].get('tokens') for cb in enabled_codebooks if cb in info['vq_info']):
                    with torch.no_grad():
                        print(f"\n{'='*80}")
                        print(f"[Step {global_step}] Caption Comparison (Active codebooks: {enabled_codebooks}):")
                        print(f"{'='*80}")
                        
                        idx = np.random.randint(0, len(captions))
                        
                        print(f"\nğŸ“ Real Caption:")
                        print(f"   {captions[idx]}")
                        
                        tokens_dict = {}
                        for codebook_name in enabled_codebooks:
                            if codebook_name in info['vq_info'] and info['vq_info'][codebook_name].get('tokens'):
                                tokens_dict[codebook_name] = [info['vq_info'][codebook_name]['tokens'][idx]]
                        
                        generated_caption = caption_generator.tokens_to_caption(tokens_dict)
                        print(f"\nğŸ¤– Generated Caption from Tokens:")
                        print(f"   {generated_caption}")
                        print(f"{'='*80}\n")
                
                # å®šæœŸä¿å­˜å’Œè¯„ä¼°
                if global_step % config['training']['sample_interval'] == 0:
                    with torch.no_grad():
                        # ç æœ¬ä½¿ç”¨ç‡ï¼ˆåªæ˜¾ç¤ºå¯ç”¨çš„ç æœ¬ï¼‰
                        print(f"\nğŸ“ˆ Step {global_step} - Codebook Usage:")
                        for name in enabled_codebooks:
                            if name in model.codebooks:
                                usage = model.codebooks[name]._get_usage_ratio()
                                perplexity = info['vq_info'][name]['perplexity'].item()
                                print(f"  {name}: {usage:.2%} (perplexity: {perplexity:.1f})")
                        
                        # ä¿å­˜æ ·æœ¬
                        if fixed_test_images is not None:
                            model.eval()
                            test_reconstructed, _ = model(fixed_test_images, return_tokens=False)
                            model.train()
                            
                            save_samples(model, fixed_test_images, test_reconstructed, 
                                       os.path.join(config['output_dir'], f'fixed_samples_step_{global_step}.png'))
                            
                            psnr = calculate_psnr(fixed_test_images, test_reconstructed)
                            ssim = calculate_ssim(fixed_test_images, test_reconstructed)
                            print(f"  ğŸ“Š Fixed Test Images - PSNR: {psnr:.2f}, SSIM: {ssim:.3f}")
                        else:
                            save_samples(model, images[:8], reconstructed[:8], 
                                       os.path.join(config['output_dir'], f'samples_step_{global_step}.png'))
                            
                            psnr = calculate_psnr(images, reconstructed)
                            ssim = calculate_ssim(images, reconstructed)
                            print(f"  ğŸ“Š PSNR: {psnr:.2f}, SSIM: {ssim:.3f}\n")
                
                global_step += 1
                
                # å®šæœŸæ¸…ç†æ˜¾å­˜
                if global_step % 50 == 0:
                    torch.cuda.empty_cache()
                    gc.collect()
        
        # epochç»“æŸåçš„å¤„ç†
        avg_epoch_loss = np.mean(epoch_losses)
        print(f"\nğŸ“Š Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}")
        
        # æ›´æ–°æœ€ä½³æŸå¤±
        if avg_epoch_loss < best_loss:
            best_loss = avg_epoch_loss
            print(f"ğŸ‰ New best loss: {best_loss:.4f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % config['training']['checkpoint_interval'] == 0:
            torch.cuda.empty_cache()
            gc.collect()
            
            # æ”¶é›†ç æœ¬ç»Ÿè®¡ä¿¡æ¯ï¼ˆåªä¿å­˜å¯ç”¨çš„ç æœ¬ï¼‰
            codebook_stats = {}
            for name in enabled_codebooks:
                if name in model.codebooks:
                    codebook_stats[name] = {
                        'usage_count': model.codebooks[name].usage_count.cpu(),
                        'total_count': model.codebooks[name].total_count.cpu()
                    }
            
            checkpoint = {
                'epoch': epoch,
                'global_step': global_step,
                'best_loss': best_loss,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': {
                    'g_optimizer': g_optimizer.state_dict(),
                },
                'scheduler_state_dict': g_scheduler.state_dict(),
                'codebook_stats': codebook_stats,
                'config': config,
                'enabled_codebooks': enabled_codebooks  # ä¿å­˜å¯ç”¨çš„ç æœ¬ä¿¡æ¯
            }
            
            if model.use_gan and d_optimizer:
                checkpoint['optimizer_state_dict']['d_optimizer'] = d_optimizer.state_dict()
            
            checkpoint_path = os.path.join(config['output_dir'], f'checkpoint_epoch_{epoch+1}.pt')
            torch.save(checkpoint, checkpoint_path)
            
            # åŒæ—¶ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_epoch_loss == best_loss:
                best_path = os.path.join(config['output_dir'], 'best_model.pt')
                torch.save(checkpoint, best_path)
                print(f"ğŸ’¾ Saved best model to {best_path}")
            
            del checkpoint
            torch.cuda.empty_cache()
            gc.collect()
            
            print(f"âœ… Saved checkpoint for epoch {epoch+1}")

# ================== ä¸»ç¨‹åº ==================
if __name__ == "__main__":
    # é…ç½®
    config = {
        'model': {
            # â­ æ–°å¢ï¼šæŒ‡å®šè¦ä½¿ç”¨çš„ç æœ¬
            # 'enabled_codebooks': ['global'],  # åªä½¿ç”¨ global ç æœ¬
            # 'enabled_codebooks': ['global', 'object'],  # ä½¿ç”¨ä¸¤ä¸ªç æœ¬
            # 'enabled_codebooks': ['global', 'object', 'color'],  # ä½¿ç”¨ä¸‰ä¸ªç æœ¬
            'enabled_codebooks': ['global', 'object', 'color', 'texture'],  # ä½¿ç”¨å…¨éƒ¨
            
            'encoder': {
                'base_channels': 64
            },
            'decoder': {
                'base_channels': 64
            },
            'discriminator_channels': 32,
            'codebooks': {
                'global': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/global',
                    'embedding_dim': 768,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                },
                'object': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/object',
                    'embedding_dim': 512,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                },
                'color': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/color',
                    'embedding_dim': 256,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                },
                'spatial': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/spatial',
                    'embedding_dim': 384,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                },
                'texture': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/texture',
                    'embedding_dim': 256,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                }
            }
        },
        'data': {
            'train_dir': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/train/data',
            'image_size': 128,
            'num_workers': 4,
            'max_samples': None,
            'fixed_test_images': [
                '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/train/data/000000000072.jpg',
                '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/train/data/000000581118.jpg',
                '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/train/data/000000576981.jpg',
                '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/train/data/000000495079.jpg',
            ]
        },
        'training': {
            'batch_size': 16,
            'num_epochs': 1000,
            'learning_rate': 0.00005,
            'use_gan': False,
            'gan_loss_type': 'standard',
            'gan_start_step': 5000,
            'd_steps_per_g': 1,
            'gradient_clip': 1.0,
            'loss_weights': {
                'reconstruction': 1.0,
                'perceptual': 0.1,
                'vq': 1.0,
                'gan': 0.1
            },
            'sample_interval': 5000,
            'checkpoint_interval': 100,
            'caption_print_interval': 2000
        },
        'output_dir': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/VQ_GAN5/no_spatial'
    }
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å‡å°‘æ˜¾å­˜ç¢ç‰‡
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
    
    # æŒ‡å®šè¦æ¢å¤çš„checkpointè·¯å¾„ï¼ˆå¯é€‰ï¼‰
    resume_checkpoint = None  # è®¾ç½®ä¸ºNoneä»å¤´å¼€å§‹è®­ç»ƒ
    # resume_checkpoint = "/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/VQ_GAN5/outputs/checkpoint_epoch_60.pt"
    
    # å¼€å§‹è®­ç»ƒ
    train_customizable_semantic_vqgan(config, resume_from=resume_checkpoint)
    
    
