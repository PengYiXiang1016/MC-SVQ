
    
# train_qformer_llama3_memory_optimized_fixed4.py - ä¿®å¤è¯„ä¼°æŒ‡æ ‡é—®é¢˜

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import os
import re
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import matplotlib.pyplot as plt
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import gc
from collections import defaultdict
import math
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
from torch.cuda.amp import autocast, GradScaler
from peft import LoraConfig, get_peft_model, TaskType

# è¯„ä¼°æŒ‡æ ‡
try:
    from pycocoevalcap.bleu.bleu import Bleu
    from pycocoevalcap.meteor.meteor import Meteor
    from pycocoevalcap.cider.cider import Cider
    from pycocoevalcap.rouge.rouge import Rouge
    METRICS_AVAILABLE = True
except:
    print("Warning: COCO evaluation metrics not available")
    METRICS_AVAILABLE = False

# å¯¼å…¥VQGANç›¸å…³ç±»
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# å¯¼å…¥VQGANç›¸å…³ç±»
from VQ_GAN5.train import (
    FixedSemanticVQGAN,
    CaptionGenerator
)

# ================== æ–‡æœ¬æ¸…æ´—å·¥å…·å‡½æ•° ==================
def clean_caption_text(text: str) -> str:
    """æ¸…æ´—å’Œæ ‡å‡†åŒ–æ–‡æœ¬"""
    if not isinstance(text, str):
        text = str(text)
    
    # ç§»é™¤ç‰¹æ®Štoken
    text = re.sub(r'<[^>]*>', '', text)
    
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    
    # ç§»é™¤éASCIIå­—ç¬¦ï¼ˆä¿ç•™åŸºæœ¬æ ‡ç‚¹ï¼‰
    text = re.sub(r'[^\x00-\x7F]+', '', text)
    
    # ç¡®ä¿ä»¥å¥å·ç»“å°¾
    if text and not text.endswith(('.', '!', '?')):
        text += '.'
    
    # é™åˆ¶é•¿åº¦
    words = text.split()
    if len(words) > 30:  # é™åˆ¶æœ€å¤§è¯æ•°
        text = ' '.join(words[:30]) + '.'
    
    return text.strip()

def safe_decode_text(text: Any) -> str:
    """å®‰å…¨è§£ç æ–‡æœ¬ï¼Œå¤„ç†å„ç§ç¼–ç é—®é¢˜"""
    if isinstance(text, bytes):
        try:
            text = text.decode('utf-8', errors='ignore')
        except:
            text = str(text)
    elif not isinstance(text, str):
        text = str(text)
    
    return clean_caption_text(text)

# ================== å†…å­˜ä¼˜åŒ–çš„QFormeræ¨¡å—ï¼ˆä¿æŒä¸å˜ï¼‰==================
class MemoryEfficientMultiHeadCrossAttention(nn.Module):
    """å†…å­˜ä¼˜åŒ–çš„å¤šå¤´äº¤å‰æ³¨æ„åŠ›æ¨¡å—"""
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # ä½¿ç”¨å•ä¸ªçº¿æ€§å±‚å‡å°‘å‚æ•°
        self.qkv_proj = nn.Linear(d_model, 3 * d_model)
        self.o_proj = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)
        
    def forward(self, query: torch.Tensor, key_value: torch.Tensor, 
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size = query.size(0)
        seq_len_q = query.size(1)
        seq_len_kv = key_value.size(1)
        
        # æ®‹å·®è¿æ¥
        residual = query
        
        # è®¡ç®—Q
        Q = self.qkv_proj(query)[:, :, :self.d_model]
        Q = Q.view(batch_size, seq_len_q, self.n_heads, self.d_k).transpose(1, 2)
        
        # è®¡ç®—Kå’ŒVï¼ˆä»key_valueï¼‰
        KV = self.qkv_proj(key_value)[:, :, self.d_model:]
        K, V = KV.chunk(2, dim=-1)
        K = K.view(batch_size, seq_len_kv, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len_kv, self.n_heads, self.d_k).transpose(1, 2)
        
        # ä½¿ç”¨Flash Attentionï¼ˆå¦‚æœå¯ç”¨ï¼‰æˆ–æ ‡å‡†æ³¨æ„åŠ›
        with autocast(enabled=False):  # æ³¨æ„åŠ›è®¡ç®—ä½¿ç”¨float32
            scores = torch.matmul(Q.float(), K.transpose(-2, -1).float()) / math.sqrt(self.d_k)
            
            if mask is not None:
                scores = scores.masked_fill(mask == 0, -1e9)
            
            attn_weights = F.softmax(scores, dim=-1)
            attn_weights = self.dropout(attn_weights)
            
            context = torch.matmul(attn_weights, V.float()).to(query.dtype)
        
        # åˆå¹¶å¤šå¤´
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)
        output = self.o_proj(context)
        
        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        output = self.layer_norm(output + residual)
        
        return output

class MemoryEfficientQFormerBlock(nn.Module):
    """å†…å­˜ä¼˜åŒ–çš„QFormerå—"""
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        
        # è‡ªæ³¨æ„åŠ›
        self.self_attn = nn.MultiheadAttention(
            d_model, n_heads, dropout=dropout, batch_first=True
        )
        self.norm1 = nn.LayerNorm(d_model)
        
        # äº¤å‰æ³¨æ„åŠ›
        self.cross_attn = MemoryEfficientMultiHeadCrossAttention(d_model, n_heads, dropout)
        self.norm2 = nn.LayerNorm(d_model)
        
        # å‰é¦ˆç½‘ç»œ - ä½¿ç”¨GLUå˜ä½“å‡å°‘å‚æ•°
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff // 2),
            nn.GELU(),
            nn.Linear(d_ff // 2, d_model),
            nn.Dropout(dropout)
        )
        self.norm3 = nn.LayerNorm(d_model)
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        self.use_checkpoint = True
        
    def _forward_impl(self, x: torch.Tensor, visual_features: torch.Tensor, 
                      mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # è‡ªæ³¨æ„åŠ›
        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=mask)
        x = self.norm1(x + attn_out)
        
        # äº¤å‰æ³¨æ„åŠ›
        cross_out = self.cross_attn(x, visual_features)
        x = self.norm2(x + cross_out)
        
        # å‰é¦ˆç½‘ç»œ
        ffn_out = self.ffn(x)
        x = self.norm3(x + ffn_out)
        
        return x
    
    def forward(self, x: torch.Tensor, visual_features: torch.Tensor, 
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        if self.use_checkpoint and self.training:
            from torch.utils.checkpoint import checkpoint
            return checkpoint(self._forward_impl, x, visual_features, mask, use_reentrant=False)
        else:
            return self._forward_impl(x, visual_features, mask)

class MemoryEfficientQFormer(nn.Module):
    """å†…å­˜ä¼˜åŒ–çš„QFormer"""
    def __init__(self, config: dict):
        super().__init__()
        
        self.config = config
        self.d_model = config['d_model']
        self.n_heads = config['n_heads']
        self.n_layers = config['n_layers']
        self.n_queries = config['n_queries']
        
        # å¯å­¦ä¹ çš„æŸ¥è¯¢å‘é‡ï¼ˆå‡å°‘æ•°é‡ï¼‰
        self.queries = nn.Parameter(torch.randn(1, self.n_queries, self.d_model) * 0.02)
        
        # å…±äº«çš„ç æœ¬æŠ•å½±å±‚
        self.shared_projection = nn.Sequential(
            nn.Linear(max(config['codebook_dims'].values()), self.d_model),
            nn.LayerNorm(self.d_model),
            nn.GELU()
        )
        
        # å„ç æœ¬çš„é€‚é…å±‚ï¼ˆå°å‹ï¼‰
        self.codebook_adapters = nn.ModuleDict()
        for name, dim in config['codebook_dims'].items():
            self.codebook_adapters[name] = nn.Linear(dim, max(config['codebook_dims'].values()))
        
        # QFormerå—
        self.blocks = nn.ModuleList([
            MemoryEfficientQFormerBlock(self.d_model, self.n_heads, self.d_model * 2)
            for _ in range(self.n_layers)
        ])
        
        # è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Linear(self.d_model, config['llama_dim'])
        
    def forward(self, codebook_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å¾
        if not codebook_features:
            raise ValueError("No codebook features provided")
            
        batch_size = next(iter(codebook_features.values())).size(0)
        device = next(iter(codebook_features.values())).device
        
        # å¤„ç†å„ç æœ¬ç‰¹å¾
        visual_features_list = []
        
        for name, features in codebook_features.items():
            if features is None:
                continue
                
            # å…ˆé€šè¿‡é€‚é…å±‚
            adapted = self.codebook_adapters[name](features)
            
            # å¦‚æœæ˜¯2Dç‰¹å¾ï¼ˆå¦‚å…¨å±€ç‰¹å¾ï¼‰ï¼Œæ·»åŠ åºåˆ—ç»´åº¦
            if len(adapted.shape) == 2:
                adapted = adapted.unsqueeze(1)
            
            # é€šè¿‡å…±äº«æŠ•å½±å±‚
            projected = self.shared_projection(adapted)
            visual_features_list.append(projected)
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        if visual_features_list:
            visual_features = torch.cat(visual_features_list, dim=1)
        else:
            # å¦‚æœæ²¡æœ‰ç‰¹å¾ï¼Œåˆ›å»ºè™šæ‹Ÿç‰¹å¾
            visual_features = torch.zeros(batch_size, 1, self.d_model, device=device)
        
        # æ¸…ç†ä¸­é—´å˜é‡
        del visual_features_list
        torch.cuda.empty_cache()
        
        # æ‰©å±•æŸ¥è¯¢å‘é‡
        queries = self.queries.expand(batch_size, -1, -1)
        
        # é€šè¿‡QFormerå—å¤„ç†
        x = queries
        for block in self.blocks:
            x = block(x, visual_features)
        
        # æŠ•å½±åˆ°LLaMAç»´åº¦
        visual_embeds = self.output_projection(x)
        
        return visual_embeds

# ================== ä½¿ç”¨LoRAçš„å›¾åƒæè¿°æ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬ ==================
class VQGANLLaMA3CaptionModelWithLoRA(nn.Module):
    """ä½¿ç”¨LoRAè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒçš„æ¨¡å‹ - ä¿®å¤ç”Ÿæˆé•¿åº¦é—®é¢˜"""
    def __init__(self, vqgan_config: dict, qformer_config: dict, llama_model_path: str):
        super().__init__()
        
        # VQGANé…ç½®
        full_vqgan_config = {
            'model': vqgan_config['model'],
            'training': {
                'use_gan': False,
                'gan_loss_type': 'standard',
                'gan_start_step': 5000,
                'd_steps_per_g': 1,
                'loss_weights': {
                    'reconstruction': 1.0,
                    'perceptual': 0.1,
                    'vq': 1.0,
                    'gan': 0.1
                }
            }
        }
        
        # åŠ è½½VQGAN
        self.vqgan = FixedSemanticVQGAN(full_vqgan_config)
        self.vqgan.eval()
        for param in self.vqgan.parameters():
            param.requires_grad = False
        
        # å†…å­˜ä¼˜åŒ–çš„QFormer
        self.qformer = MemoryEfficientQFormer(qformer_config)
        
        # åŠ è½½LLaMA3 with 4-bit quantization
        print("Loading LLaMA3 model with 4-bit quantization...")
        self.tokenizer = AutoTokenizer.from_pretrained(llama_model_path)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # 4-bité‡åŒ–é…ç½®
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        self.llama = AutoModelForCausalLM.from_pretrained(
            llama_model_path,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16
        )
        
        # é…ç½®LoRA
        lora_config = LoraConfig(
            r=16,  # rank
            lora_alpha=32,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )
        
        # åº”ç”¨LoRA
        self.llama = get_peft_model(self.llama, lora_config)
        self.llama.print_trainable_parameters()
        
        # æ·»åŠ è§†è§‰tokenå’Œç‰¹æ®ŠæŒ‡ä»¤token
        special_tokens = {
            'additional_special_tokens': ['<visual>', '<brief>', '</brief>']
        }
        self.tokenizer.add_special_tokens(special_tokens)
        self.visual_token_id = self.tokenizer.convert_tokens_to_ids("<visual>")
        self.brief_start_token_id = self.tokenizer.convert_tokens_to_ids("<brief>")
        self.brief_end_token_id = self.tokenizer.convert_tokens_to_ids("</brief>")
        self.llama.resize_token_embeddings(len(self.tokenizer))
        
        # è§†è§‰æŠ•å½±å±‚ï¼ˆè¿æ¥QFormerå’ŒLLaMAï¼‰
        self.visual_projection = nn.Sequential(
            nn.Linear(qformer_config['n_queries'] * qformer_config['llama_dim'], 
                     qformer_config['llama_dim']),
            nn.LayerNorm(qformer_config['llama_dim']),
            nn.Dropout(0.1)
        )
        
    def extract_visual_features(self, images: torch.Tensor) -> Dict[str, torch.Tensor]:
        """æå–VQGANå¤šç æœ¬ç‰¹å¾ - ä¿®å¤ç‰ˆæœ¬v2"""
        with torch.no_grad():
            batch_size = images.size(0)
            
            # åˆ†æ‰¹å¤„ç†ä»¥èŠ‚çœæ˜¾å­˜
            if batch_size > 4:
                all_features = defaultdict(list)
                
                for i in range(0, batch_size, 4):
                    batch_images = images[i:i+4]
                    _, info = self.vqgan(batch_images, return_tokens=False)
                    
                    # ä»quantized_featuresæå–ç‰¹å¾
                    if 'quantized_features' in info:
                        for name, features in info['quantized_features'].items():
                            if features is not None:
                                all_features[name].append(features)
                
                # åˆå¹¶æ‰¹æ¬¡
                codebook_features = {}
                for name, features_list in all_features.items():
                    if features_list:
                        codebook_features[name] = torch.cat(features_list, dim=0)
                
                # æ¸…ç†
                del all_features
                torch.cuda.empty_cache()
                
            else:
                _, info = self.vqgan(images, return_tokens=False)
                
                # ä»quantized_featuresæå–ç‰¹å¾
                if 'quantized_features' in info:
                    codebook_features = {}
                    
                    for name, features in info['quantized_features'].items():
                        if features is not None:
                            # ç¡®ä¿ç‰¹å¾ç»´åº¦æ­£ç¡®
                            if len(features.shape) == 4:  # [B, C, H, W]
                                if name == 'global':
                                    # å…¨å±€ç‰¹å¾ä½¿ç”¨å¹³å‡æ± åŒ–
                                    features = F.adaptive_avg_pool2d(features, 1).squeeze(-1).squeeze(-1)
                                else:
                                    # å…¶ä»–ç‰¹å¾å±•å¹³ç©ºé—´ç»´åº¦
                                    b, c, h, w = features.shape
                                    features = features.permute(0, 2, 3, 1).reshape(b, h * w, c)
                            codebook_features[name] = features
                else:
                    # å¦‚æœquantized_featuresä¸å­˜åœ¨ï¼Œå°è¯•ä»vq_infoæå–
                    codebook_features = {}
                    
                    if 'vq_info' in info:
                        for name in ['global', 'object', 'color', 'spatial', 'texture']:
                            if name in info['vq_info']:
                                vq_info = info['vq_info'][name]
                                
                                # å°è¯•ä¸åŒçš„é”®å
                                quantized = None
                                if isinstance(vq_info, dict):
                                    for key in ['z_q', 'quantized', 'features', 'embeddings']:
                                        if key in vq_info:
                                            quantized = vq_info[key]
                                            break
                                
                                if quantized is not None:
                                    # å¤„ç†ä¸åŒå½¢çŠ¶çš„ç‰¹å¾
                                    if len(quantized.shape) == 4:  # [B, C, H, W]
                                        if name == 'global':
                                            quantized = F.adaptive_avg_pool2d(quantized, 1).squeeze(-1).squeeze(-1)
                                        else:
                                            b, c, h, w = quantized.shape
                                            quantized = quantized.permute(0, 2, 3, 1).reshape(b, h * w, c)
                                    elif len(quantized.shape) == 2 and name != 'global':  # [B, C]
                                        quantized = quantized.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦
                                    
                                    codebook_features[name] = quantized
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€äº›ç‰¹å¾
        if not codebook_features:
            # åˆ›å»ºè™šæ‹Ÿç‰¹å¾ä½œä¸ºåå¤‡
            device = images.device
            batch_size = images.size(0)
            for name, dim in self.qformer.config['codebook_dims'].items():
                if name == 'global':
                    codebook_features[name] = torch.randn(batch_size, dim, device=device)
                else:
                    codebook_features[name] = torch.randn(batch_size, 16, dim, device=device)  # 16æ˜¯åºåˆ—é•¿åº¦
        
        return codebook_features
    
    def forward(self, images: torch.Tensor, captions: List[str], 
                max_length: int = 32) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå¸¦ç®€çŸ­æ ‡é¢˜æŒ‡å¯¼"""
        batch_size = images.size(0)
        device = images.device
        
        # ä½¿ç”¨æ··åˆç²¾åº¦
        with autocast():
            # 1. æå–è§†è§‰ç‰¹å¾
            codebook_features = self.extract_visual_features(images)
            
            # 2. é€šè¿‡QFormerè·å¾—è§†è§‰åµŒå…¥
            visual_embeds = self.qformer(codebook_features)  # [B, n_queries, llama_dim]
            
            # æ¸…ç†ä¸­é—´å˜é‡
            del codebook_features
            torch.cuda.empty_cache()
            
            # 3. å‡†å¤‡å¸¦æŒ‡å¯¼çš„æ–‡æœ¬è¾“å…¥ - å¼ºè°ƒç®€çŸ­æè¿°
            prompts = []
            for _ in range(batch_size):
                # ä½¿ç”¨æ›´æ˜ç¡®çš„æŒ‡ä»¤å¼•å¯¼æ¨¡å‹ç”Ÿæˆç®€çŸ­æè¿°
                prompt = "<visual> <brief>Describe the image in one short sentence:"
                prompts.append(prompt)
            
            # Tokenize
            prompt_tokens = self.tokenizer(
                prompts, 
                return_tensors="pt", 
                padding=True,
                truncation=True,
                max_length=20  # é™åˆ¶prompté•¿åº¦
            ).to(device)
            
            caption_tokens = self.tokenizer(
                captions,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=max_length,  # ä½¿ç”¨æ›´çŸ­çš„æœ€å¤§é•¿åº¦
                add_special_tokens=False  # ä¸æ·»åŠ é¢å¤–çš„ç‰¹æ®Štoken
            ).to(device)
            
            # æ·»åŠ </brief>ç»“æŸæ ‡è®°
            brief_end_tokens = torch.full(
                (batch_size, 1), 
                self.brief_end_token_id, 
                dtype=torch.long, 
                device=device
            )
            
            # 4. å¤„ç†è§†è§‰åµŒå…¥
            visual_embeds_flat = visual_embeds.reshape(batch_size, -1)  # [B, n_queries * llama_dim]
            visual_embed_projected = self.visual_projection(visual_embeds_flat)  # [B, llama_dim]
            
            # 5. æ„å»ºè¾“å…¥åµŒå…¥
            prompt_embeds = self.llama.get_input_embeddings()(prompt_tokens.input_ids)
            
            # åœ¨<visual>ä½ç½®æ’å…¥è§†è§‰åµŒå…¥
            visual_token_mask = prompt_tokens.input_ids == self.visual_token_id
            for i in range(batch_size):
                visual_pos = visual_token_mask[i].nonzero(as_tuple=True)[0]
                if len(visual_pos) > 0:
                    prompt_embeds[i, visual_pos[0]] = visual_embed_projected[i]
            
            # 6. å‡†å¤‡å®Œæ•´è¾“å…¥ï¼ˆåŒ…å«ç»“æŸæ ‡è®°ï¼‰
            full_embeds = torch.cat([
                prompt_embeds,
                self.llama.get_input_embeddings()(caption_tokens.input_ids),
                self.llama.get_input_embeddings()(brief_end_tokens)
            ], dim=1)
            
            full_attention_mask = torch.cat([
                prompt_tokens.attention_mask,
                caption_tokens.attention_mask,
                torch.ones_like(brief_end_tokens)
            ], dim=1)
            
            # æ„å»ºæ ‡ç­¾
            labels = torch.cat([
                torch.full_like(prompt_tokens.input_ids, -100),
                caption_tokens.input_ids,
                brief_end_tokens
            ], dim=1)
        
        # 7. å‰å‘ä¼ æ’­LLaMAï¼ˆä¸ä½¿ç”¨æ··åˆç²¾åº¦ï¼‰
        outputs = self.llama(
            inputs_embeds=full_embeds,
            attention_mask=full_attention_mask,
            labels=labels,
            return_dict=True
        )
        
        # æ·»åŠ é•¿åº¦æƒ©ç½šï¼ˆé¼“åŠ±ç”ŸæˆçŸ­å¥ï¼‰
        caption_lengths = (caption_tokens.attention_mask.sum(dim=1).float() - 1).clamp(min=1)
        length_penalty = torch.where(
            caption_lengths > 15,  # å¦‚æœè¶…è¿‡15ä¸ªtoken
            (caption_lengths - 15) * 0.01,  # æ·»åŠ æƒ©ç½š
            torch.zeros_like(caption_lengths)
        )
        
        total_loss = outputs.loss + length_penalty.mean()
        
        return {
            'loss': total_loss,
            'logits': outputs.logits,
            'base_loss': outputs.loss,
            'length_penalty': length_penalty.mean()
        }
    
    @torch.no_grad()
    def generate_caption(self, images: torch.Tensor, max_length: int = 25,  # å‡å°‘æœ€å¤§é•¿åº¦
                        temperature: float = 0.7, top_p: float = 0.9) -> List[str]:
        """ç”Ÿæˆå›¾åƒæè¿° - ä¿®å¤ç‰ˆæœ¬ï¼Œç”Ÿæˆç®€çŸ­æè¿°"""
        batch_size = images.size(0)
        device = images.device
        
        # æå–è§†è§‰ç‰¹å¾
        codebook_features = self.extract_visual_features(images)
        visual_embeds = self.qformer(codebook_features)
        
        # æ¸…ç†
        del codebook_features
        torch.cuda.empty_cache()
        
        # å‡†å¤‡å¼•å¯¼prompt
        prompts = []
        for _ in range(batch_size):
            # ä½¿ç”¨æ˜ç¡®çš„æŒ‡ä»¤ç”Ÿæˆç®€çŸ­æè¿°
            prompt = "<visual> <brief>Describe the image in one short sentence:"
            prompts.append(prompt)
            
        prompt_tokens = self.tokenizer(
            prompts, 
            return_tensors="pt", 
            padding=True,
            truncation=True,
            max_length=20
        ).to(device)
        
        # æ„å»ºè¾“å…¥åµŒå…¥
        visual_embeds_flat = visual_embeds.reshape(batch_size, -1)
        visual_embed_projected = self.visual_projection(visual_embeds_flat)
        
        prompt_embeds = self.llama.get_input_embeddings()(prompt_tokens.input_ids)
        visual_token_mask = prompt_tokens.input_ids == self.visual_token_id
        
        for i in range(batch_size):
            visual_pos = visual_token_mask[i].nonzero(as_tuple=True)[0]
            if len(visual_pos) > 0:
                prompt_embeds[i, visual_pos[0]] = visual_embed_projected[i]
        
        # ç”Ÿæˆæ—¶ä½¿ç”¨æ›´ä¸¥æ ¼çš„å‚æ•°
        with autocast():
            outputs = self.llama.generate(
                inputs_embeds=prompt_embeds,
                max_new_tokens=max_length,
                min_new_tokens=5,  # è‡³å°‘ç”Ÿæˆ5ä¸ªtoken
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=[self.tokenizer.eos_token_id, self.brief_end_token_id],  # é‡åˆ°</brief>ä¹Ÿåœæ­¢
                repetition_penalty=1.2,  # å‡å°‘é‡å¤
            )
        
        # è§£ç 
        captions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
        
        # æ¸…ç†ç”Ÿæˆçš„caption
        cleaned_captions = []
        for caption in captions:
            # ç§»é™¤promptéƒ¨åˆ†
            caption = caption.replace("<visual>", "").replace("<brief>", "")
            caption = caption.replace("Describe the image in one short sentence:", "").strip()
            
            # ä½¿ç”¨æ–‡æœ¬æ¸…æ´—å‡½æ•°
            caption = clean_caption_text(caption)
            
            cleaned_captions.append(caption)
        
        return cleaned_captions

# ================== ä¿®å¤åçš„æ•°æ®é›†ç±»ï¼ˆä¿æŒä¸å˜ï¼‰==================
class COCOCaptionDataset(Dataset):
    """COCO Captionæ•°æ®é›† - ä¿®å¤collateé—®é¢˜"""
    def __init__(self, root_dir: str, ann_file: str, transform=None, max_samples: Optional[int] = None):
        self.root_dir = root_dir
        self.transform = transform
        
        # åŠ è½½æ ‡æ³¨æ–‡ä»¶
        with open(ann_file, 'r') as f:
            self.coco = json.load(f)
        
        # æ„å»ºå›¾åƒIDåˆ°æ ‡æ³¨çš„æ˜ å°„
        self.img_id_to_anns = defaultdict(list)
        for ann in self.coco['annotations']:
            # æ¸…æ´—æ ‡æ³¨æ–‡æœ¬
            clean_caption = clean_caption_text(ann['caption'])
            if clean_caption:  # åªæ·»åŠ éç©ºçš„æ ‡æ³¨
                self.img_id_to_anns[ann['image_id']].append(clean_caption)
        
        # æ„å»ºå›¾åƒIDåˆ°æ–‡ä»¶åçš„æ˜ å°„
        self.img_id_to_filename = {}
        for img in self.coco['images']:
            self.img_id_to_filename[img['id']] = img['file_name']
        
        # è·å–æ‰€æœ‰æœ‰æ ‡æ³¨çš„å›¾åƒIDï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªæ¸…æ´—åçš„æ ‡æ³¨ï¼‰
        self.img_ids = [img_id for img_id, anns in self.img_id_to_anns.items() if anns]
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if max_samples is not None:
            self.img_ids = self.img_ids[:max_samples]
        
        print(f"Loaded {len(self.img_ids)} images with clean captions")
    
    def __len__(self):
        return len(self.img_ids)
    
    def __getitem__(self, idx):
        img_id = self.img_ids[idx]
        
        # åŠ è½½å›¾åƒ
        img_filename = self.img_id_to_filename[img_id]
        img_path = os.path.join(self.root_dir, img_filename)
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        # è·å–æ‰€æœ‰æ¸…æ´—åçš„æ ‡æ³¨
        captions = self.img_id_to_anns[img_id]
        
        # éšæœºé€‰æ‹©ä¸€ä¸ªæ ‡æ³¨ç”¨äºè®­ç»ƒ
        caption = np.random.choice(captions)
        
        return image, caption, img_id

# è‡ªå®šä¹‰collateå‡½æ•°
def custom_collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°å¤„ç†ä¸åŒé•¿åº¦çš„æ ‡æ³¨"""
    images = torch.stack([item[0] for item in batch])
    captions = [item[1] for item in batch]
    img_ids = [item[2] for item in batch]
    
    return images, captions, img_ids

# ================== ä¿®å¤çš„è¯„ä¼°æŒ‡æ ‡è®¡ç®—å‡½æ•° ==================
def compute_metrics_safe(generated_captions: List[str], reference_captions: List[List[str]]) -> Dict[str, float]:
    """å®‰å…¨è®¡ç®—BLEU, METEOR, CIDErç­‰æŒ‡æ ‡ - ä¿®å¤æ‰€æœ‰å·²çŸ¥é—®é¢˜"""
    if not METRICS_AVAILABLE:
        return {'avg_length': np.mean([len(cap.split()) for cap in generated_captions])}
    
    # å½»åº•æ¸…æ´—æ‰€æœ‰æ–‡æœ¬
    cleaned_generated = []
    cleaned_references = []
    
    for gen_cap, ref_caps in zip(generated_captions, reference_captions):
        # æ¸…æ´—ç”Ÿæˆçš„æ–‡æœ¬
        gen_cap_clean = safe_decode_text(gen_cap)
        if not gen_cap_clean or len(gen_cap_clean.strip()) == 0:
            gen_cap_clean = "a photo."  # æœ€ç®€å•çš„åå¤‡caption
        
        # æ¸…æ´—å‚è€ƒæ–‡æœ¬
        ref_caps_clean = []
        for ref_cap in ref_caps:
            ref_cap_clean = safe_decode_text(ref_cap)
            if ref_cap_clean and len(ref_cap_clean.strip()) > 0:
                ref_caps_clean.append(ref_cap_clean)
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå‚è€ƒæ–‡æœ¬
        if not ref_caps_clean:
            ref_caps_clean = ["a photo."]
        
        cleaned_generated.append(gen_cap_clean)
        cleaned_references.append(ref_caps_clean)
    
    # å‡†å¤‡æ•°æ®æ ¼å¼
    gts = {}
    res = {}
    
    for i, (gen_cap, ref_caps) in enumerate(zip(cleaned_generated, cleaned_references)):
        gts[i] = ref_caps
        res[i] = [gen_cap]
    
    metrics = {}
    
    # æ·»åŠ åŸºæœ¬ç»Ÿè®¡
    gen_lengths = [len(cap.split()) for cap in cleaned_generated]
    metrics['avg_gen_length'] = np.mean(gen_lengths)
    metrics['std_gen_length'] = np.std(gen_lengths)
    
    # BLEU (ç›¸å¯¹å®‰å…¨)
    try:
        bleu_scorer = Bleu(4)
        bleu_scores, _ = bleu_scorer.compute_score(gts, res)
        for i, score in enumerate(bleu_scores):
            metrics[f'BLEU-{i+1}'] = float(score)
    except Exception as e:
        print(f"BLEU computation failed: {e}")
        # æä¾›ç®€å•çš„n-gramé‡å è®¡ç®—ä½œä¸ºåå¤‡
        metrics['BLEU-1'] = 0.0
        metrics['BLEU-4'] = 0.0
    
    # CIDEr (é€šå¸¸æ¯”è¾ƒç¨³å®š)
    try:
        cider_scorer = Cider()
        cider_score, _ = cider_scorer.compute_score(gts, res)
        metrics['CIDEr'] = float(cider_score)
    except Exception as e:
        print(f"CIDEr computation failed: {e}")
        metrics['CIDEr'] = 0.0
    
    # ROUGE (ç›¸å¯¹å®‰å…¨)
    try:
        rouge_scorer = Rouge()
        rouge_score, _ = rouge_scorer.compute_score(gts, res)
        metrics['ROUGE-L'] = float(rouge_score)
    except Exception as e:
        print(f"ROUGE computation failed: {e}")
        metrics['ROUGE-L'] = 0.0
    
    # è·³è¿‡METEORï¼Œå› ä¸ºå®ƒç»å¸¸å‡ºç°ç¼–ç é—®é¢˜
    print("Skipping METEOR computation due to encoding issues")
    
    return metrics

# ================== ä¼˜åŒ–çš„è®­ç»ƒå‡½æ•° - ä¿®å¤ç‰ˆæœ¬ ==================
def train_qformer_llama3_memory_optimized(config: dict):
    """å†…å­˜ä¼˜åŒ–çš„è®­ç»ƒå‡½æ•° - ä¿®å¤ç‰ˆæœ¬"""
    
    # è®¾ç½®è®¾å¤‡å’Œå†…å­˜ä¼˜åŒ–
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # å¯ç”¨cudnn benchmark
    torch.backends.cudnn.benchmark = True
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['output_dir'], exist_ok=True)
    
    # æ•°æ®åŠ è½½
    transform = transforms.Compose([
        transforms.Resize((config['data']['image_size'], config['data']['image_size'])),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # è®­ç»ƒé›†
    train_dataset = COCOCaptionDataset(
        root_dir=config['data']['train_dir'],
        ann_file=config['data']['train_ann_file'],
        transform=transform,
        max_samples=config['data'].get('max_samples')
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=True,
        num_workers=config['data']['num_workers'],
        pin_memory=True,
        drop_last=True,
        persistent_workers=True,  # ä¿æŒworkersæ´»è·ƒ
        collate_fn=custom_collate_fn  # ä½¿ç”¨è‡ªå®šä¹‰collateå‡½æ•°
    )
    
    # éªŒè¯é›†ï¼ˆç”¨äºè¯„ä¼°æŒ‡æ ‡ï¼‰
    if config['data'].get('val_ann_file'):
        val_dataset = COCOCaptionDataset(
            root_dir=config['data']['val_dir'],
            ann_file=config['data']['val_ann_file'],
            transform=transform,
            max_samples=50  # åªç”¨50ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€ŸéªŒè¯ï¼Œå‡å°‘å‡ºé”™æ¦‚ç‡
        )
    
    # åˆ›å»ºæ¨¡å‹
    print("Creating model...")
    model = VQGANLLaMA3CaptionModelWithLoRA(
        vqgan_config=config['vqgan'],
        qformer_config=config['qformer'],
        llama_model_path=config['llama_model_path']
    )
    
    # åŠ è½½VQGAN checkpoint
    if config['vqgan_checkpoint']:
        print(f"Loading VQGAN checkpoint from {config['vqgan_checkpoint']}")
        vqgan_state = torch.load(config['vqgan_checkpoint'], map_location='cpu')
        model.vqgan.load_state_dict(vqgan_state['model_state_dict'], strict=False)
        del vqgan_state
        gc.collect()
    
    model = model.to(device)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Trainable parameters: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)")
    
    # ä¼˜åŒ–å™¨ - åˆ†ç»„å­¦ä¹ ç‡
    optimizer_grouped_parameters = [
        {
            'params': model.qformer.parameters(),
            'lr': config['training']['learning_rate']
        },
        {
            'params': model.visual_projection.parameters(),
            'lr': config['training']['learning_rate']
        },
        {
            'params': model.llama.parameters(),
            'lr': config['training']['learning_rate'] * 0.1  # LoRAå‚æ•°ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡
        }
    ]
    
    optimizer = torch.optim.AdamW(
        optimizer_grouped_parameters,
        weight_decay=config['training']['weight_decay']
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler()
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    num_training_steps = len(train_loader) * config['training']['num_epochs']
    num_warmup_steps = int(0.1 * num_training_steps)
    
    from transformers import get_linear_schedule_with_warmup
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps
    )
    
    # æ¢¯åº¦ç´¯ç§¯
    gradient_accumulation_steps = config['training'].get('gradient_accumulation_steps', 4)
    
    # è®­ç»ƒå¾ªç¯
    print("\nğŸš€ Starting training...")
    global_step = 0
    
    for epoch in range(config['training']['num_epochs']):
        model.train()
        epoch_losses = []
        epoch_length_penalties = []
        
        with tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['training']['num_epochs']}") as pbar:
            for batch_idx, (images, captions, img_ids) in enumerate(pbar):
                images = images.to(device)
                
                # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                with autocast():
                    outputs = model(images, captions)
                    loss = outputs['loss'] / gradient_accumulation_steps
                
                # åå‘ä¼ æ’­
                scaler.scale(loss).backward()
                
                # æ¢¯åº¦ç´¯ç§¯
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    # æ¢¯åº¦è£å‰ª
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config['training']['gradient_clip'])
                    
                    # ä¼˜åŒ–å™¨æ­¥éª¤
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
                    scheduler.step()
                    
                    # è®°å½•æŸå¤±
                    epoch_losses.append(outputs['base_loss'].item())
                    if 'length_penalty' in outputs:
                        epoch_length_penalties.append(outputs['length_penalty'].item())
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.set_postfix({
                        'loss': f"{outputs['base_loss'].item():.4f}",
                        'len_pen': f"{outputs.get('length_penalty', 0).item():.4f}",
                        'lr': f"{optimizer.param_groups[0]['lr']:.2e}"
                    })
                
                # å®šæœŸæ‰“å°ç”Ÿæˆæ ·ä¾‹å’Œè¯„ä¼°æŒ‡æ ‡
                if global_step % config['training']['print_interval'] == 0 and global_step > 0:
                    model.eval()
                    with torch.no_grad():
                        # åªä½¿ç”¨2ä¸ªæ ·æœ¬ç”Ÿæˆä»¥èŠ‚çœæ˜¾å­˜
                        sample_images = images[:2]
                        sample_captions = captions[:2]
                        
                        generated_captions = model.generate_caption(sample_images)
                        
                        print(f"\n{'='*80}")
                        print(f"[Step {global_step}] Caption Comparison:")
                        print(f"{'='*80}")
                        
                        for i in range(len(generated_captions)):
                            print(f"\nğŸ“¸ Image {i+1}:")
                            print(f"ğŸ“ Real Caption: {sample_captions[i]}")
                            print(f"ğŸ¤– Generated Caption: {generated_captions[i]}")
                            print(f"ğŸ“ Generated Length: {len(generated_captions[i].split())} words")
                        
                        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆåœ¨éªŒè¯é›†ä¸Šï¼‰- ä½¿ç”¨å®‰å…¨ç‰ˆæœ¬
                        if METRICS_AVAILABLE and config['data'].get('val_ann_file'):
                            print("\nğŸ“Š Computing metrics on validation set...")
                            val_generated = []
                            val_references = []
                            
                            try:
                                # åœ¨éªŒè¯é›†çš„ä¸€å°éƒ¨åˆ†ä¸Šè¯„ä¼°
                                for i in range(min(10, len(val_dataset))):  # è¿›ä¸€æ­¥å‡å°‘æ ·æœ¬æ•°é‡
                                    val_img, _, val_img_id = val_dataset[i]
                                    val_img = val_img.unsqueeze(0).to(device)
                                    
                                    gen_cap = model.generate_caption(val_img)[0]
                                    ref_caps = val_dataset.img_id_to_anns[val_img_id]
                                    
                                    val_generated.append(gen_cap)
                                    val_references.append(ref_caps)
                                
                                # ä½¿ç”¨å®‰å…¨çš„æŒ‡æ ‡è®¡ç®—å‡½æ•°
                                metrics = compute_metrics_safe(val_generated, val_references)
                                
                                print("\nğŸ“ˆ Validation Metrics:")
                                for metric_name, score in metrics.items():
                                    print(f"  {metric_name}: {score:.4f}")
                                    
                            except Exception as e:
                                print(f"Metrics computation failed: {e}")
                                print("Continuing training without metrics...")
                        
                        print(f"{'='*80}\n")
                    
                    model.train()
                
                global_step += 1
                
                # å®šæœŸæ¸…ç†æ˜¾å­˜
                if global_step % 50 == 0:
                    torch.cuda.empty_cache()
                    gc.collect()
        
        # Epochç»“æŸåçš„å¤„ç†
        avg_epoch_loss = np.mean(epoch_losses) if epoch_losses else 0
        avg_length_penalty = np.mean(epoch_length_penalties) if epoch_length_penalties else 0
        print(f"\nğŸ“Š Epoch {epoch+1} completed.")
        print(f"   Average loss: {avg_epoch_loss:.4f}")
        print(f"   Average length penalty: {avg_length_penalty:.4f}")
        
        # ä¿å­˜checkpoint
        if (epoch + 1) % config['training']['checkpoint_interval'] == 0:
            torch.save({
                'epoch': epoch,
                'global_step': global_step,
                'qformer_state_dict': model.qformer.state_dict(),
                'visual_projection_state_dict': model.visual_projection.state_dict(),
                'llama_lora_state_dict': model.llama.state_dict(),  # LoRA weights
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'config': config
            }, os.path.join(config['output_dir'], f'checkpoint_epoch_{epoch+1}.pt'))
            print(f"âœ… Saved checkpoint for epoch {epoch+1}")
            
            # æ¸…ç†
            torch.cuda.empty_cache()
            gc.collect()

# ================== ä¸»ç¨‹åº ==================
if __name__ == "__main__":
    # é…ç½®
    config = {
        # VQGANé…ç½®
        'vqgan': {
            'model': {
                'encoder': {
                    'base_channels': 64
                },
                'decoder': {
                    'base_channels': 64
                },
                'discriminator_channels': 32,
                'codebooks': {
                    'global': {
                        'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/global',
                        'embedding_dim': 768,
                        'commitment_cost': 0.25,
                        'decay': 0.99,
                        'epsilon': 1e-5
                    },
                    'object': {
                        'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/object',
                        'embedding_dim': 512,
                        'commitment_cost': 0.25,
                        'decay': 0.99,
                        'epsilon': 1e-5
                    },
                    'color': {
                        'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/color',
                        'embedding_dim': 256,
                        'commitment_cost': 0.25,
                        'decay': 0.99,
                        'epsilon': 1e-5
                    },
                    'spatial': {
                        'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/spatial',
                        'embedding_dim': 384,
                        'commitment_cost': 0.25,
                        'decay': 0.99,
                        'epsilon': 1e-5
                    },
                    'texture': {
                        'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/texture',
                        'embedding_dim': 256,
                        'commitment_cost': 0.25,
                        'decay': 0.99,
                        'epsilon': 1e-5
                    }
                }
            }
        },
        
        # VQGAN checkpointè·¯å¾„
        'vqgan_checkpoint': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/VQ_GAN5/outputs/checkpoint_epoch_60.pt',
        
        # QFormeré…ç½®ï¼ˆå‡å°è§„æ¨¡ï¼‰
        'qformer': {
            'd_model': 512,  # å‡å°éšè—ç»´åº¦
            'n_heads': 8,    # å‡å°‘æ³¨æ„åŠ›å¤´æ•°
            'n_layers': 4,   # å‡å°‘å±‚æ•°
            'n_queries': 16, # å‡å°‘æŸ¥è¯¢æ•°é‡
            'codebook_dims': {
                'global': 768,
                'object': 512,
                'color': 256,
                'spatial': 384,
                'texture': 256
            },
            'llama_dim': 4096
        },
        
        # LLaMA3æ¨¡å‹è·¯å¾„
        'llama_model_path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/Meta-Llama-3-8B-Instruct',  # è¯·æ›¿æ¢ä¸ºå®é™…è·¯å¾„
        
        # æ•°æ®é…ç½®
        'data': {
            'train_dir': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/train/data',
            'train_ann_file': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/raw/captions_train2017.json',  # è¯·æ›¿æ¢ä¸ºå®é™…è·¯å¾„
            'val_dir': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/validation/data',
            'val_ann_file': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/raw/captions_val2017.json',  # è¯·æ›¿æ¢ä¸ºå®é™…è·¯å¾„
            'image_size': 128,
            'num_workers': 2,  # å‡å°‘workeræ•°é‡
            'max_samples': 120000,  # å¯ä»¥å…ˆç”¨è¾ƒå°‘æ•°æ®æµ‹è¯•
        },
        
        # è®­ç»ƒé…ç½®
        'training': {
            'batch_size': 2,  # å¤§å¹…å‡å°æ‰¹æ¬¡å¤§å°
            'gradient_accumulation_steps': 8,  # ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
            'num_epochs': 10,
            'learning_rate': 2e-4,
            'weight_decay': 0.01,
            'gradient_clip': 1.0,
            'print_interval': 10000,
            'checkpoint_interval': 2,
        },
        
        # è¾“å‡ºç›®å½•
        'output_dir': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/Former4/checkpoint'
    }
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
    
    # å¼€å§‹è®­ç»ƒ
    train_qformer_llama3_memory_optimized(config)