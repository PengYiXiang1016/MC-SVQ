
# test_codebook_attention_heatmap.py - ä¿®å¤ç‰ˆæœ¬2
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from PIL import Image
import os
from typing import Dict, List, Tuple, Optional
from torchvision import transforms
import seaborn as sns
from matplotlib.colors import LinearSegmentedColormap

# å¯¼å…¥æ¨¡å‹ç›¸å…³çš„ç±»
from train import (
    FixedSemanticCodebook,
    ImprovedSemanticEncoder,
    ImprovedSemanticDecoder,
    PatchDiscriminator,
    FixedSemanticVQGAN,
    CaptionGenerator
)

class CodebookAttentionVisualizer:
    """ç æœ¬æ³¨æ„åŠ›å¯è§†åŒ–å™¨"""
    
    def __init__(self, model: FixedSemanticVQGAN, device: torch.device):
        self.model = model
        self.device = device
        self.model.eval()
        
        # ä¸ºæ¯ä¸ªç æœ¬å®šä¹‰é¢œè‰²æ–¹æ¡ˆ
        self.codebook_colors = {
            'global': 'Blues',
            'object': 'Reds', 
            'color': 'Greens',
            'spatial': 'Purples',
            'texture': 'Oranges'
        }
        
    def extract_codebook_features(self, image: torch.Tensor) -> Dict[str, Dict]:
        """æå–æ¯ä¸ªç æœ¬çš„ç‰¹å¾å’Œæ³¨æ„åŠ›ä¿¡æ¯"""
        with torch.no_grad():
            # é€šè¿‡å®Œæ•´çš„forward passè·å–ä¿¡æ¯
            reconstructed, info = self.model(image, return_tokens=True)
            
            # é€šè¿‡ç¼–ç å™¨è·å–ç‰¹å¾
            encoded = self.model.encoder(image)
            
            # å­˜å‚¨æ¯ä¸ªç æœ¬çš„ä¿¡æ¯
            codebook_info = {}
            
            # æ£€æŸ¥encodedçš„ç±»å‹
            if isinstance(encoded, dict):
                # å¦‚æœç¼–ç å™¨è¿”å›å­—å…¸ï¼Œä½¿ç”¨å¯¹åº”çš„ç‰¹å¾
                for name, codebook in self.model.codebooks.items():
                    if name in encoded:
                        features = encoded[name]
                    else:
                        # å¦‚æœæ²¡æœ‰å¯¹åº”çš„ç‰¹å¾ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„ç‰¹å¾
                        features = list(encoded.values())[0] if encoded else None
                        
                    if features is None:
                        print(f"Warning: No features found for {name} codebook")
                        continue
                        
                    # å¤„ç†ç‰¹å¾
                    codebook_info[name] = self._process_codebook_features(
                        features, codebook, name, info
                    )
            else:
                # å¦‚æœç¼–ç å™¨è¿”å›å•ä¸ªå¼ é‡ï¼Œä¸ºæ‰€æœ‰ç æœ¬ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾
                features = encoded
                
                for name, codebook in self.model.codebooks.items():
                    codebook_info[name] = self._process_codebook_features(
                        features, codebook, name, info
                    )
                
        return codebook_info
    
    def _process_codebook_features(self, features: torch.Tensor, 
                                 codebook: FixedSemanticCodebook,
                                 codebook_name: str,
                                 forward_info: Dict) -> Dict:
        """å¤„ç†å•ä¸ªç æœ¬çš„ç‰¹å¾"""
        B, C, H, W = features.shape
        features_flat = features.permute(0, 2, 3, 1).contiguous().view(-1, C)
        
        # è·å–ç æœ¬çš„embeddings - ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨codebook.embeddings
        embeddings = codebook.embeddings
        
        # å¦‚æœç‰¹å¾ç»´åº¦å’Œç æœ¬ç»´åº¦ä¸åŒ¹é…ï¼Œéœ€è¦æŠ•å½±
        if C != embeddings.shape[1]:
            # åˆ›å»ºæŠ•å½±å±‚
            if not hasattr(self, f'projection_{codebook_name}'):
                projection = nn.Linear(C, embeddings.shape[1]).to(self.device)
                # åˆå§‹åŒ–æŠ•å½±å±‚
                nn.init.xavier_uniform_(projection.weight)
                setattr(self, f'projection_{codebook_name}', projection)
            else:
                projection = getattr(self, f'projection_{codebook_name}')
            
            with torch.no_grad():
                features_flat = projection(features_flat)
        
        # è®¡ç®—è·ç¦»
        distances = torch.cdist(features_flat, embeddings)
        
        # è·å–æœ€è¿‘çš„ç æœ¬ç´¢å¼•
        encoding_indices = torch.argmin(distances, dim=1)
        
        # è®¡ç®—è½¯æ³¨æ„åŠ›æƒé‡ï¼ˆä½¿ç”¨è´Ÿè·ç¦»çš„softmaxï¼‰
        attention_weights = F.softmax(-distances / 0.1, dim=1)  # æ·»åŠ æ¸©åº¦å‚æ•°
        
        # è·å–æœ€å¤§æ³¨æ„åŠ›æƒé‡ä½œä¸ºè¯¥ä½ç½®çš„æ¿€æ´»å¼ºåº¦
        max_attention = torch.max(attention_weights, dim=1)[0]
        max_attention = max_attention.view(B, H, W)
        
        # è·å–ä½¿ç”¨çš„ç æœ¬ç´¢å¼•
        encoding_indices = encoding_indices.view(B, H, W)
        
        # å¦‚æœforward_infoä¸­æœ‰å¯¹åº”çš„ä¿¡æ¯ï¼Œä½¿ç”¨å®ƒ
        if 'vq_info' in forward_info and codebook_name in forward_info['vq_info']:
            vq_info = forward_info['vq_info'][codebook_name]
            if 'encodings' in vq_info:
                # ä½¿ç”¨å®é™…çš„ç¼–ç ä¿¡æ¯
                actual_encodings = vq_info['encodings']
                if actual_encodings.shape[-2:] == (H, W):
                    encoding_indices = actual_encodings
                    
        return {
            'attention_map': max_attention,
            'encoding_indices': encoding_indices,
            'distances': distances.view(B, H, W, -1),
            'features_shape': (B, C, H, W)
        }
    
    def create_attention_heatmap(self, 
                               image: torch.Tensor,
                               codebook_info: Dict[str, Dict],
                               save_path: Optional[str] = None) -> plt.Figure:
        """åˆ›å»ºæ³¨æ„åŠ›çƒ­åŠ›å›¾å¯è§†åŒ–"""
        
        # å°†å›¾åƒè½¬æ¢ä¸ºnumpyå¹¶åå½’ä¸€åŒ–
        image_np = image.squeeze(0).cpu().numpy()
        image_np = (image_np * 0.5 + 0.5).transpose(1, 2, 0)
        image_np = np.clip(image_np, 0, 1)
        
        # åˆ›å»ºå›¾å½¢å¸ƒå±€
        fig = plt.figure(figsize=(20, 12))
        gs = gridspec.GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.2)
        
        # 1. æ˜¾ç¤ºåŸå§‹å›¾åƒ
        ax_orig = fig.add_subplot(gs[0, 0])
        ax_orig.imshow(image_np)
        ax_orig.set_title('Original Image', fontsize=14, fontweight='bold')
        ax_orig.axis('off')
        
        # 2. ä¸ºæ¯ä¸ªç æœ¬åˆ›å»ºçƒ­åŠ›å›¾
        positions = [(0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]
        
        for idx, (name, info) in enumerate(codebook_info.items()):
            if idx < len(positions):
                row, col = positions[idx]
                ax = fig.add_subplot(gs[row, col])
                
                # è·å–æ³¨æ„åŠ›å›¾
                attention_map = info['attention_map'].squeeze(0).cpu().numpy()
                
                # è·å–ç‰¹å¾å›¾çš„å°ºå¯¸
                _, _, feat_h, feat_w = info['features_shape']
                
                # å¦‚æœæ³¨æ„åŠ›å›¾å°ºå¯¸ä¸å›¾åƒä¸åŒï¼Œè¿›è¡Œä¸Šé‡‡æ ·
                if attention_map.shape != (image_np.shape[0], image_np.shape[1]):
                    attention_map_resized = F.interpolate(
                        torch.tensor(attention_map).unsqueeze(0).unsqueeze(0),
                        size=(image_np.shape[0], image_np.shape[1]),
                        mode='bilinear',
                        align_corners=False
                    ).squeeze().numpy()
                else:
                    attention_map_resized = attention_map
                
                # åˆ›å»ºçƒ­åŠ›å›¾
                im = ax.imshow(attention_map_resized, 
                             cmap=self.codebook_colors[name],
                             alpha=0.8)
                
                # å åŠ åŸå§‹å›¾åƒ
                ax.imshow(image_np, alpha=0.3)
                
                ax.set_title(f'{name.capitalize()} Codebook Attention\n(Feature size: {feat_h}Ã—{feat_w})', 
                           fontsize=12, fontweight='bold')
                ax.axis('off')
                
                # æ·»åŠ é¢œè‰²æ¡
                cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
                cbar.ax.tick_params(labelsize=8)
        
        # 3. åˆ›å»ºç»„åˆçƒ­åŠ›å›¾
        ax_combined = fig.add_subplot(gs[2, :])
        
        # ä¸ºæ¯ä¸ªç æœ¬åˆ†é…ä¸åŒçš„é¢œè‰²é€šé“
        combined_map = np.zeros((image_np.shape[0], image_np.shape[1], 3))
        
        color_channels = {
            'global': [0, 0, 1],    # è“è‰²
            'object': [1, 0, 0],    # çº¢è‰²
            'color': [0, 1, 0],     # ç»¿è‰²
            'spatial': [1, 0, 1],   # ç´«è‰²
            'texture': [1, 0.5, 0]  # æ©™è‰²
        }
        
        for name, info in codebook_info.items():
            attention_map = info['attention_map'].squeeze(0).cpu().numpy()
            
            # ä¸Šé‡‡æ ·åˆ°å›¾åƒå¤§å°
            if attention_map.shape != (image_np.shape[0], image_np.shape[1]):
                attention_map_resized = F.interpolate(
                    torch.tensor(attention_map).unsqueeze(0).unsqueeze(0),
                    size=(image_np.shape[0], image_np.shape[1]),
                    mode='bilinear',
                    align_corners=False
                ).squeeze().numpy()
            else:
                attention_map_resized = attention_map
            
            # å½’ä¸€åŒ–
            if attention_map_resized.max() > 0:
                attention_map_resized = attention_map_resized / attention_map_resized.max()
            
            # æ·»åŠ åˆ°ç»„åˆå›¾
            color = color_channels.get(name, [1, 1, 1])
            for i in range(3):
                combined_map[:, :, i] += attention_map_resized * color[i] * 0.3
        
        # å½’ä¸€åŒ–ç»„åˆå›¾
        combined_map = np.clip(combined_map, 0, 1)
        
        # æ˜¾ç¤ºç»„åˆå›¾
        ax_combined.imshow(image_np)
        ax_combined.imshow(combined_map, alpha=0.6)
        ax_combined.set_title('Combined Codebook Attention Map', 
                            fontsize=14, fontweight='bold')
        ax_combined.axis('off')
        
        # æ·»åŠ å›¾ä¾‹
        from matplotlib.patches import Patch
        legend_elements = [Patch(facecolor=color_channels[name], 
                               label=name.capitalize(), 
                               alpha=0.6) 
                         for name in codebook_info.keys()]
        ax_combined.legend(handles=legend_elements, 
                          loc='center left', 
                          bbox_to_anchor=(1, 0.5),
                          fontsize=10)
        
        plt.suptitle('Multi-Codebook Semantic Attention Analysis', 
                    fontsize=16, fontweight='bold')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… Saved attention heatmap to: {save_path}")
        
        return fig
    
    def analyze_codebook_usage(self, 
                             codebook_info: Dict[str, Dict]) -> Dict[str, Dict]:
        """åˆ†æç æœ¬ä½¿ç”¨æƒ…å†µ"""
        usage_stats = {}
        
        for name, info in codebook_info.items():
            indices = info['encoding_indices'].cpu().numpy().flatten()
            unique_indices, counts = np.unique(indices, return_counts=True)
            
            usage_stats[name] = {
                'num_unique_codes': len(unique_indices),
                'most_used_codes': unique_indices[np.argsort(counts)[-5:]][::-1],
                'usage_distribution': counts / counts.sum()
            }
            
        return usage_stats


def load_model_from_checkpoint(checkpoint_path: str, config: dict, device: torch.device):
    """ä»checkpointåŠ è½½æ¨¡å‹"""
    model = FixedSemanticVQGAN(config).to(device)
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    print(f"âœ… Model loaded from: {checkpoint_path}")
    return model


def test_single_image(image_path: str, 
                     model: FixedSemanticVQGAN,
                     output_dir: str,
                     device: torch.device):
    """æµ‹è¯•å•å¼ å›¾ç‰‡"""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((128, 128)),  # ä½¿ç”¨è®­ç»ƒæ—¶çš„å›¾åƒå¤§å°
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # åŠ è½½å›¾åƒ
    image = Image.open(image_path).convert('RGB')
    image_tensor = transform(image).unsqueeze(0).to(device)
    
    print(f"\nğŸ–¼ï¸  Processing image: {os.path.basename(image_path)}")
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = CodebookAttentionVisualizer(model, device)
    
    # æå–ç æœ¬ç‰¹å¾
    print("ğŸ“Š Extracting codebook features...")
    codebook_info = visualizer.extract_codebook_features(image_tensor)
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    print("ğŸ¨ Creating attention heatmap...")
    output_path = os.path.join(output_dir, 
                              f'attention_heatmap_{os.path.basename(image_path)}')
    fig = visualizer.create_attention_heatmap(image_tensor, 
                                            codebook_info, 
                                            save_path=output_path)
    plt.show()
    
    # åˆ†æç æœ¬ä½¿ç”¨æƒ…å†µ
    print("\nğŸ“ˆ Codebook usage statistics:")
    usage_stats = visualizer.analyze_codebook_usage(codebook_info)
    for name, stats in usage_stats.items():
        print(f"\n{name.capitalize()} codebook:")
        print(f"  - Unique codes used: {stats['num_unique_codes']}")
        print(f"  - Most used codes: {stats['most_used_codes']}")
    
    # ç”Ÿæˆé‡å»ºå›¾åƒè¿›è¡Œå¯¹æ¯”
    print("\nğŸ”„ Generating reconstruction...")
    with torch.no_grad():
        reconstructed, info = model(image_tensor, return_tokens=True)
    
    # ä¿å­˜é‡å»ºå¯¹æ¯”å›¾
    fig_recon = plt.figure(figsize=(10, 5))
    
    # åŸå§‹å›¾åƒ
    ax1 = fig_recon.add_subplot(1, 2, 1)
    image_np = image_tensor.squeeze(0).cpu().numpy()
    image_np = (image_np * 0.5 + 0.5).transpose(1, 2, 0)
    ax1.imshow(np.clip(image_np, 0, 1))
    ax1.set_title('Original', fontsize=12)
    ax1.axis('off')
    
    # é‡å»ºå›¾åƒ
    ax2 = fig_recon.add_subplot(1, 2, 2)
    recon_np = reconstructed.squeeze(0).cpu().numpy()
    recon_np = (recon_np * 0.5 + 0.5).transpose(1, 2, 0)
    ax2.imshow(np.clip(recon_np, 0, 1))
    ax2.set_title('Reconstruction', fontsize=12)
    ax2.axis('off')
    
    plt.suptitle('Original vs Reconstruction', fontsize=14)
    recon_path = os.path.join(output_dir, 
                             f'reconstruction_{os.path.basename(image_path)}')
    plt.savefig(recon_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"âœ… Reconstruction saved to: {recon_path}")
    
    # å¦‚æœæœ‰tokenä¿¡æ¯ï¼Œç”Ÿæˆcaption
    if 'vq_info' in info:
        caption_generator = CaptionGenerator()
        tokens_dict = {}
        
        for codebook_name in ['global', 'object', 'color', 'spatial', 'texture']:
            if codebook_name in info['vq_info'] and info['vq_info'][codebook_name].get('tokens'):
                tokens_dict[codebook_name] = info['vq_info'][codebook_name]['tokens']
        
        if tokens_dict:
            generated_caption = caption_generator.tokens_to_caption(tokens_dict)
            print(f"\nğŸ¤– Generated Caption from Tokens:")
            print(f"   {generated_caption}")


if __name__ == "__main__":
    # é…ç½®ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
    config = {
        'model': {
            'encoder': {
                'base_channels': 64
            },
            'decoder': {
                'base_channels': 64
            },
            'discriminator_channels': 32,
            'codebooks': {
                'global': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/global',
                    'embedding_dim': 768,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                },
                'object': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/object',
                    'embedding_dim': 512,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                },
                'color': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/color',
                    'embedding_dim': 256,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                },
                'spatial': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/spatial',
                    'embedding_dim': 384,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                },
                'texture': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/texture',
                    'embedding_dim': 256,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                }
            }
        },
        'data': {
            'image_size': 128
        },
        'training': {
            'use_gan': False  # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
        }
    }
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # åŠ è½½æ¨¡å‹
    checkpoint_path = "/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/VQ_GAN5/outputs/checkpoint_epoch_60.pt"
    model = load_model_from_checkpoint(checkpoint_path, config, device)
    
    # æµ‹è¯•å›¾åƒè·¯å¾„
    test_images = [
        '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/train/data/000000581881.jpg',
        # å¯ä»¥æ·»åŠ æ›´å¤šæµ‹è¯•å›¾åƒ
    ]
    
    # è¾“å‡ºç›®å½•
    output_dir = '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/VQ_GAN5/attention'
    
    # æµ‹è¯•æ¯å¼ å›¾åƒ
    for image_path in test_images:
        if os.path.exists(image_path):
            test_single_image(image_path, model, output_dir, device)
        else:
            print(f"âŒ Image not found: {image_path}")