
# # enhanced_semantic_vqgan_resume_fixed2.py - ä¿®å¤GANè®­ç»ƒçš„è®¡ç®—å›¾é—®é¢˜

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import numpy as np
# import json
# import os
# from typing import Dict, List, Tuple, Optional
# from dataclasses import dataclass
# import matplotlib.pyplot as plt
# from tqdm import tqdm
# import wandb
# from torch.utils.data import DataLoader, Dataset
# from torchvision import transforms
# from PIL import Image
# import lpips
# from torchvision.models import vgg16
# import glob
# from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
# from collections import Counter
# import gc

# # å¯¼å…¥åŸæœ‰çš„æ‰€æœ‰ç±»ï¼ˆä¿æŒä¸å˜ï¼‰
# from train import (
#     FixedSemanticCodebook,
#     ImprovedSemanticEncoder,
#     ImprovedSemanticDecoder,
#     PatchDiscriminator,
#     FixedSemanticVQGAN,
#     COCOImageDataset,
#     CaptionGenerator,
#     save_samples,
#     calculate_psnr,
#     calculate_ssim
# )

# # ================== ä¿®å¤GANè®­ç»ƒçš„è®­ç»ƒå‡½æ•° ==================
# def train_fixed_semantic_vqgan_resume(config: dict, resume_from: Optional[str] = None):
#     """æ”¯æŒä»checkpointæ¢å¤çš„è®­ç»ƒå‡½æ•° - ä¿®å¤GANè®­ç»ƒçš„è®¡ç®—å›¾é—®é¢˜"""
    
#     # è®¾ç½®è®¾å¤‡
#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#     print(f"Using device: {device}")
    
#     # åˆ›å»ºè¾“å‡ºç›®å½•
#     os.makedirs(config['output_dir'], exist_ok=True)
    
#     # Captionç”Ÿæˆå™¨
#     caption_generator = CaptionGenerator()
    
#     # æ•°æ®åŠ è½½
#     transform = transforms.Compose([
#         transforms.Resize((config['data']['image_size'], config['data']['image_size'])),
#         transforms.ToTensor(),
#         transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
#     ])
    
#     dataset = COCOImageDataset(
#         root_dir=config['data']['train_dir'],
#         transform=transform,
#         max_samples=config['data'].get('max_samples')
#     )
    
#     dataloader = DataLoader(
#         dataset,
#         batch_size=config['training']['batch_size'],
#         shuffle=True,
#         num_workers=config['data']['num_workers'],
#         pin_memory=True,
#         drop_last=True
#     )
    
#     # åˆ›å»ºæ¨¡å‹
#     model = FixedSemanticVQGAN(config).to(device)
    
#     # æ‰“å°æ¨¡å‹å‚æ•°é‡å’Œå„ç æœ¬ç»´åº¦
#     total_params = sum(p.numel() for p in model.parameters())
#     print(f"Total model parameters: {total_params:,}")
#     print("\nCodebook dimensions:")
#     for name, cb_config in config['model']['codebooks'].items():
#         print(f"  {name}: {cb_config.get('embedding_dim', 256)}")
    
#     # ä¼˜åŒ–å™¨
#     g_params = list(model.encoder.parameters()) + list(model.decoder.parameters()) + list(model.codebooks.parameters())
#     g_optimizer = torch.optim.AdamW(g_params, lr=config['training']['learning_rate'], betas=(0.5, 0.999))
    
#     d_optimizer = None
#     if model.use_gan:
#         d_optimizer = torch.optim.AdamW(
#             model.discriminator.parameters(),
#             lr=config['training']['learning_rate'],
#             betas=(0.5, 0.999)
#         )
    
#     # å­¦ä¹ ç‡è°ƒåº¦å™¨
#     g_scheduler = CosineAnnealingWarmRestarts(g_optimizer, T_0=len(dataloader), T_mult=2)
    
#     # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
#     start_epoch = 0
#     global_step = 0
#     best_loss = float('inf')
    
#     # å¦‚æœæŒ‡å®šäº†æ¢å¤çš„checkpointï¼ˆä¿æŒåŸæœ‰çš„æ¢å¤é€»è¾‘ï¼‰
#     if resume_from and os.path.exists(resume_from):
#         print(f"\nğŸ”„ Resuming training from: {resume_from}")
#         checkpoint = torch.load(resume_from, map_location=device)
        
#         # æ™ºèƒ½åŠ è½½æ¨¡å‹çŠ¶æ€
#         model_state = checkpoint['model_state_dict']
#         current_state = model.state_dict()
        
#         matched_keys = []
#         missing_keys = []
        
#         for key in current_state.keys():
#             if key in model_state and current_state[key].shape == model_state[key].shape:
#                 matched_keys.append(key)
#             else:
#                 missing_keys.append(key)
        
#         filtered_state = {k: v for k, v in model_state.items() if k in matched_keys}
#         model.load_state_dict(filtered_state, strict=False)
        
#         print(f"âœ… Loaded {len(matched_keys)} matching keys")
        
#         if missing_keys:
#             print(f"âš ï¸  Missing keys: {len(missing_keys)}")
#             if any('discriminator' in key for key in missing_keys):
#                 print("ğŸ“Œ Note: Discriminator weights missing - will initialize randomly")
        
#         # æ¢å¤å…¶ä»–çŠ¶æ€
#         if 'optimizer_state_dict' in checkpoint:
#             try:
#                 g_optimizer.load_state_dict(checkpoint['optimizer_state_dict']['g_optimizer'])
#                 print("âœ… Generator optimizer state loaded")
#             except:
#                 print("âš ï¸  Using fresh generator optimizer")
        
#         if 'epoch' in checkpoint:
#             start_epoch = checkpoint['epoch'] + 1
#         if 'global_step' in checkpoint:
#             global_step = checkpoint.get('global_step', 0)
#         if 'best_loss' in checkpoint:
#             best_loss = checkpoint.get('best_loss', float('inf'))
        
#         print(f"\nğŸ“Š Resuming from epoch {start_epoch}, global step {global_step}")
        
#         del checkpoint
#         gc.collect()
#         torch.cuda.empty_cache()
    
#     # æŸå¤±å‡½æ•°
#     perceptual_loss_fn = lpips.LPIPS(net='vgg').to(device)
#     perceptual_loss_fn.eval()
#     for param in perceptual_loss_fn.parameters():
#         param.requires_grad = False
    
#     # è®­ç»ƒå¾ªç¯
#     caption_print_interval = config['training'].get('caption_print_interval', 200)
    
#     print(f"\nğŸš€ Starting training from epoch {start_epoch}")
#     print(f"   GAN training: {'Enabled' if model.use_gan else 'Disabled'}")
    
#     for epoch in range(start_epoch, config['training']['num_epochs']):
#         model.train()
#         epoch_losses = []
        
#         with tqdm(dataloader, desc=f"Epoch {epoch+1}/{config['training']['num_epochs']}") as pbar:
#             for batch_idx, (images, captions) in enumerate(pbar):
#                 images = images.to(device)
                
#                 # å†³å®šæ˜¯å¦éœ€è¦è¿”å›tokens
#                 return_tokens = (global_step % caption_print_interval == 0)
                
#                 # GANè®­ç»ƒæ ‡å¿—
#                 train_gan = model.use_gan and d_optimizer and global_step > config['training'].get('gan_start_step', 5000)
                
#                 # === ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒåˆ¤åˆ«å™¨ï¼ˆå¦‚æœå¯ç”¨GANï¼‰ ===
#                 if train_gan and global_step % config['training']['d_steps_per_g'] == 0:
#                     # ç”Ÿæˆå‡å›¾åƒç”¨äºåˆ¤åˆ«å™¨è®­ç»ƒ
#                     with torch.no_grad():
#                         fake_images, _ = model(images, return_tokens=False)
#                         fake_images = fake_images.detach()
                    
#                     # è®­ç»ƒåˆ¤åˆ«å™¨
#                     d_optimizer.zero_grad()
                    
#                     real_pred = model.discriminator(images)
#                     fake_pred = model.discriminator(fake_images)
                    
#                     d_loss = F.relu(1.0 - real_pred).mean() + F.relu(1.0 + fake_pred).mean()
#                     d_loss.backward()
#                     d_optimizer.step()
                
#                 # === ç¬¬äºŒæ­¥ï¼šè®­ç»ƒç”Ÿæˆå™¨ ===
#                 # å‰å‘ä¼ æ’­ï¼ˆç”¨äºé‡å»ºæŸå¤±ï¼‰
#                 reconstructed, info = model(images, return_tokens=return_tokens)
                
#                 # 1. é‡å»ºæŸå¤±
#                 recon_loss = F.mse_loss(reconstructed, images) * config['training']['loss_weights']['reconstruction']
                
#                 # 2. æ„ŸçŸ¥æŸå¤±
#                 with torch.no_grad():
#                     perceptual_loss_value = perceptual_loss_fn(reconstructed, images).mean().item()
#                 perceptual_loss = perceptual_loss_value * config['training']['loss_weights']['perceptual']
                
#                 # 3. VQæŸå¤±
#                 vq_loss = sum(info['vq_losses'].values()) * config['training']['loss_weights']['vq']
                
#                 # 4. GANæŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
#                 gan_loss_value = 0.0
#                 if train_gan:
#                     # ä½¿ç”¨å½“å‰çš„reconstructedè®¡ç®—GANæŸå¤±
#                     fake_pred_g = model.discriminator(reconstructed)
#                     gan_loss = -fake_pred_g.mean() * config['training']['loss_weights']['gan']
#                     gan_loss_value = gan_loss.item()
                    
#                     # ç”Ÿæˆå™¨æ€»æŸå¤±ï¼ˆåŒ…å«GANæŸå¤±ï¼‰
#                     g_loss = recon_loss + vq_loss + perceptual_loss + gan_loss
#                 else:
#                     # ç”Ÿæˆå™¨æ€»æŸå¤±ï¼ˆä¸åŒ…å«GANæŸå¤±ï¼‰
#                     g_loss = recon_loss + vq_loss + perceptual_loss
                
#                 # ç”Ÿæˆå™¨ä¼˜åŒ–
#                 g_optimizer.zero_grad(set_to_none=True)
#                 g_loss.backward()
                
#                 # æ¢¯åº¦è£å‰ª
#                 torch.nn.utils.clip_grad_norm_(g_params, config['training']['gradient_clip'])
                
#                 g_optimizer.step()
#                 g_scheduler.step()
                
#                 # è®°å½•æŸå¤±
#                 total_loss = g_loss.item()
#                 epoch_losses.append(total_loss)
                
#                 # æ›´æ–°è¿›åº¦æ¡
#                 with torch.no_grad():
#                     pbar.set_postfix({
#                         'loss': f"{total_loss:.2f}",
#                         'recon': f"{F.mse_loss(reconstructed, images).item():.3f}",
#                         'gan': f"{gan_loss_value:.3f}",
#                         'lr': f"{g_optimizer.param_groups[0]['lr']:.2e}"
#                     })
                
#                 # æ‰“å°captionå¯¹æ¯”
#                 if return_tokens and info['vq_info']['global'].get('tokens'):
#                     with torch.no_grad():
#                         print(f"\n{'='*80}")
#                         print(f"[Step {global_step}] Caption Comparison:")
#                         print(f"{'='*80}")
                        
#                         idx = np.random.randint(0, len(captions))
                        
#                         print(f"\nğŸ“ Real Caption:")
#                         print(f"   {captions[idx]}")
                        
#                         tokens_dict = {}
#                         for codebook_name in ['global', 'object', 'color', 'spatial', 'texture']:
#                             if codebook_name in info['vq_info'] and info['vq_info'][codebook_name].get('tokens'):
#                                 tokens_dict[codebook_name] = [info['vq_info'][codebook_name]['tokens'][idx]]
                        
#                         generated_caption = caption_generator.tokens_to_caption(tokens_dict)
#                         print(f"\nğŸ¤– Generated Caption from Tokens:")
#                         print(f"   {generated_caption}")
#                         print(f"{'='*80}\n")
                
#                 # å®šæœŸä¿å­˜å’Œè¯„ä¼°
#                 if global_step % config['training']['sample_interval'] == 0:
#                     with torch.no_grad():
#                         # ç æœ¬ä½¿ç”¨ç‡
#                         print(f"\nğŸ“ˆ Step {global_step} - Codebook Usage:")
#                         for name, codebook in model.codebooks.items():
#                             usage = codebook._get_usage_ratio()
#                             perplexity = info['vq_info'][name]['perplexity'].item()
#                             print(f"  {name}: {usage:.2%} (perplexity: {perplexity:.1f})")
                        
#                         # ä¿å­˜æ ·æœ¬
#                         save_samples(model, images[:8], reconstructed[:8], 
#                                    os.path.join(config['output_dir'], f'samples_step_{global_step}.png'))
                        
#                         # è®¡ç®—PSNRå’ŒSSIM
#                         psnr = calculate_psnr(images, reconstructed)
#                         ssim = calculate_ssim(images, reconstructed)
#                         print(f"  ğŸ“Š PSNR: {psnr:.2f}, SSIM: {ssim:.3f}\n")
                
#                 global_step += 1
                
#                 # å®šæœŸæ¸…ç†æ˜¾å­˜
#                 if global_step % 50 == 0:
#                     torch.cuda.empty_cache()
#                     gc.collect()
        
#         # epochç»“æŸåçš„å¤„ç†
#         avg_epoch_loss = np.mean(epoch_losses)
#         print(f"\nğŸ“Š Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}")
        
#         # æ›´æ–°æœ€ä½³æŸå¤±
#         if avg_epoch_loss < best_loss:
#             best_loss = avg_epoch_loss
#             print(f"ğŸ‰ New best loss: {best_loss:.4f}")
        
#         # ä¿å­˜æ£€æŸ¥ç‚¹
#         if (epoch + 1) % config['training']['checkpoint_interval'] == 0:
#             torch.cuda.empty_cache()
#             gc.collect()
            
#             # æ”¶é›†ç æœ¬ç»Ÿè®¡ä¿¡æ¯
#             codebook_stats = {}
#             for name, codebook in model.codebooks.items():
#                 codebook_stats[name] = {
#                     'usage_count': codebook.usage_count.cpu(),
#                     'total_count': codebook.total_count.cpu()
#                 }
            
#             checkpoint = {
#                 'epoch': epoch,
#                 'global_step': global_step,
#                 'best_loss': best_loss,
#                 'model_state_dict': model.state_dict(),
#                 'optimizer_state_dict': {
#                     'g_optimizer': g_optimizer.state_dict(),
#                 },
#                 'scheduler_state_dict': g_scheduler.state_dict(),
#                 'codebook_stats': codebook_stats,
#                 'config': config
#             }
            
#             if model.use_gan and d_optimizer:
#                 checkpoint['optimizer_state_dict']['d_optimizer'] = d_optimizer.state_dict()
            
#             checkpoint_path = os.path.join(config['output_dir'], f'checkpoint_epoch_{epoch+1}.pt')
#             torch.save(checkpoint, checkpoint_path)
            
#             # åŒæ—¶ä¿å­˜æœ€ä½³æ¨¡å‹
#             if avg_epoch_loss == best_loss:
#                 best_path = os.path.join(config['output_dir'], 'best_model.pt')
#                 torch.save(checkpoint, best_path)
#                 print(f"ğŸ’¾ Saved best model to {best_path}")
            
#             del checkpoint
#             torch.cuda.empty_cache()
#             gc.collect()
            
#             print(f"âœ… Saved checkpoint for epoch {epoch+1}")

# # ================== ä¸»ç¨‹åº ==================
# if __name__ == "__main__":
#     # é…ç½®
#     config = {
#         'model': {
#             'encoder': {
#                 'base_channels': 64
#             },
#             'decoder': {
#                 'base_channels': 64
#             },
#             'discriminator_channels': 32,
#             'codebooks': {
#                 'global': {
#                     'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/global',
#                     'embedding_dim': 768,
#                     'commitment_cost': 0.25,
#                     'decay': 0.99,
#                     'epsilon': 1e-5
#                 },
#                 'object': {
#                     'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/object',
#                     'embedding_dim': 512,
#                     'commitment_cost': 0.25,
#                     'decay': 0.99,
#                     'epsilon': 1e-5
#                 },
#                 'color': {
#                     'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/color',
#                     'embedding_dim': 256,
#                     'commitment_cost': 0.25,
#                     'decay': 0.99,
#                     'epsilon': 1e-5
#                 },
#                 'spatial': {
#                     'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/spatial',
#                     'embedding_dim': 384,
#                     'commitment_cost': 0.25,
#                     'decay': 0.99,
#                     'epsilon': 1e-5
#                 },
#                 'texture': {
#                     'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/texture',
#                     'embedding_dim': 256,
#                     'commitment_cost': 0.25,
#                     'decay': 0.99,
#                     'epsilon': 1e-5
#                 }
#             }
#         },
#         'data': {
#             'train_dir': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/train/data',
#             'image_size': 128,
#             'num_workers': 4,
#             'max_samples': None
#         },
#         'training': {
#             'batch_size': 16,
#             'num_epochs': 200,
#             'learning_rate': 0.00005,
#             'use_gan': False,
#             'gan_loss_type': 'standard',
#             'gan_start_step': 5000,  # å¯ä»¥è®¾ç½®ä¸º0ç«‹å³å¼€å§‹GANè®­ç»ƒ
#             'd_steps_per_g': 1,
#             'gradient_clip': 1.0,
#             'loss_weights': {
#                 'reconstruction': 1.0,
#                 'perceptual': 0.1,
#                 'vq': 1.0,
#                 'gan': 0.1
#             },
#             'sample_interval': 500,
#             'checkpoint_interval': 5,
#             'caption_print_interval': 200
#         },
#         'output_dir': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/VQ_GAN5/outputs'
#     }
    
#     # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å‡å°‘æ˜¾å­˜ç¢ç‰‡
#     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
    
#     # æŒ‡å®šè¦æ¢å¤çš„checkpointè·¯å¾„
#     resume_checkpoint = "/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/VQ_GAN5/outputs/checkpoint_epoch_45.pt"
    
#     # å¼€å§‹è®­ç»ƒï¼ˆä»checkpointæ¢å¤ï¼‰
#     train_fixed_semantic_vqgan_resume(config, resume_from=resume_checkpoint)
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
# enhanced_semantic_vqgan_resume_fixed2.py - ä¿®å¤GANè®­ç»ƒçš„è®¡ç®—å›¾é—®é¢˜

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import os
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import matplotlib.pyplot as plt
from tqdm import tqdm
import wandb
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import lpips
from torchvision.models import vgg16
import glob
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from collections import Counter
import gc

# å¯¼å…¥åŸæœ‰çš„æ‰€æœ‰ç±»ï¼ˆä¿æŒä¸å˜ï¼‰
from train import (
    FixedSemanticCodebook,
    ImprovedSemanticEncoder,
    ImprovedSemanticDecoder,
    PatchDiscriminator,
    FixedSemanticVQGAN,
    COCOImageDataset,
    CaptionGenerator,
    save_samples,
    calculate_psnr,
    calculate_ssim
)

# ================== æ·»åŠ åŠ è½½å›ºå®šæµ‹è¯•å›¾åƒçš„å‡½æ•° ==================
def load_fixed_test_images(image_paths: List[str], transform, device):
    """åŠ è½½å›ºå®šçš„æµ‹è¯•å›¾åƒ"""
    test_images = []
    valid_paths = []
    
    for path in image_paths:
        if os.path.exists(path):
            try:
                img = Image.open(path).convert('RGB')
                img_tensor = transform(img).unsqueeze(0).to(device)
                test_images.append(img_tensor)
                valid_paths.append(path)
                print(f"âœ… Loaded test image: {os.path.basename(path)}")
            except Exception as e:
                print(f"âŒ Failed to load {path}: {e}")
        else:
            print(f"âŒ Test image not found: {path}")
    
    if test_images:
        # å°†æ‰€æœ‰æµ‹è¯•å›¾åƒæ‹¼æ¥æˆä¸€ä¸ªbatch
        test_images = torch.cat(test_images, dim=0)
        print(f"\nğŸ“¸ Loaded {len(valid_paths)} fixed test images for monitoring")
    else:
        test_images = None
        print("\nâš ï¸  No valid test images loaded")
    
    return test_images, valid_paths

# ================== ä¿®å¤GANè®­ç»ƒçš„è®­ç»ƒå‡½æ•° ==================
def train_fixed_semantic_vqgan_resume(config: dict, resume_from: Optional[str] = None):
    """æ”¯æŒä»checkpointæ¢å¤çš„è®­ç»ƒå‡½æ•° - ä¿®å¤GANè®­ç»ƒçš„è®¡ç®—å›¾é—®é¢˜"""
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['output_dir'], exist_ok=True)
    
    # Captionç”Ÿæˆå™¨
    caption_generator = CaptionGenerator()
    
    # æ•°æ®åŠ è½½
    transform = transforms.Compose([
        transforms.Resize((config['data']['image_size'], config['data']['image_size'])),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # åŠ è½½å›ºå®šçš„æµ‹è¯•å›¾åƒ
    fixed_test_images = None
    fixed_test_paths = []
    if 'fixed_test_images' in config['data'] and config['data']['fixed_test_images']:
        fixed_test_images, fixed_test_paths = load_fixed_test_images(
            config['data']['fixed_test_images'], 
            transform, 
            device
        )
    
    dataset = COCOImageDataset(
        root_dir=config['data']['train_dir'],
        transform=transform,
        max_samples=config['data'].get('max_samples')
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=config['training']['batch_size'],
        shuffle=True,
        num_workers=config['data']['num_workers'],
        pin_memory=True,
        drop_last=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = FixedSemanticVQGAN(config).to(device)
    
    # æ‰“å°æ¨¡å‹å‚æ•°é‡å’Œå„ç æœ¬ç»´åº¦
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total model parameters: {total_params:,}")
    print("\nCodebook dimensions:")
    for name, cb_config in config['model']['codebooks'].items():
        print(f"  {name}: {cb_config.get('embedding_dim', 256)}")
    
    # ä¼˜åŒ–å™¨
    g_params = list(model.encoder.parameters()) + list(model.decoder.parameters()) + list(model.codebooks.parameters())
    g_optimizer = torch.optim.AdamW(g_params, lr=config['training']['learning_rate'], betas=(0.5, 0.999))
    
    d_optimizer = None
    if model.use_gan:
        d_optimizer = torch.optim.AdamW(
            model.discriminator.parameters(),
            lr=config['training']['learning_rate'],
            betas=(0.5, 0.999)
        )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    g_scheduler = CosineAnnealingWarmRestarts(g_optimizer, T_0=len(dataloader), T_mult=2)
    
    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    start_epoch = 0
    global_step = 0
    best_loss = float('inf')
    
    # å¦‚æœæŒ‡å®šäº†æ¢å¤çš„checkpointï¼ˆä¿æŒåŸæœ‰çš„æ¢å¤é€»è¾‘ï¼‰
    if resume_from and os.path.exists(resume_from):
        print(f"\nğŸ”„ Resuming training from: {resume_from}")
        checkpoint = torch.load(resume_from, map_location=device)
        
        # æ™ºèƒ½åŠ è½½æ¨¡å‹çŠ¶æ€
        model_state = checkpoint['model_state_dict']
        current_state = model.state_dict()
        
        matched_keys = []
        missing_keys = []
        
        for key in current_state.keys():
            if key in model_state and current_state[key].shape == model_state[key].shape:
                matched_keys.append(key)
            else:
                missing_keys.append(key)
        
        filtered_state = {k: v for k, v in model_state.items() if k in matched_keys}
        model.load_state_dict(filtered_state, strict=False)
        
        print(f"âœ… Loaded {len(matched_keys)} matching keys")
        
        if missing_keys:
            print(f"âš ï¸  Missing keys: {len(missing_keys)}")
            if any('discriminator' in key for key in missing_keys):
                print("ğŸ“Œ Note: Discriminator weights missing - will initialize randomly")
        
        # æ¢å¤å…¶ä»–çŠ¶æ€
        if 'optimizer_state_dict' in checkpoint:
            try:
                g_optimizer.load_state_dict(checkpoint['optimizer_state_dict']['g_optimizer'])
                print("âœ… Generator optimizer state loaded")
            except:
                print("âš ï¸  Using fresh generator optimizer")
        
        if 'epoch' in checkpoint:
            start_epoch = checkpoint['epoch'] + 1
        if 'global_step' in checkpoint:
            global_step = checkpoint.get('global_step', 0)
        if 'best_loss' in checkpoint:
            best_loss = checkpoint.get('best_loss', float('inf'))
        
        print(f"\nğŸ“Š Resuming from epoch {start_epoch}, global step {global_step}")
        
        del checkpoint
        gc.collect()
        torch.cuda.empty_cache()
    
    # æŸå¤±å‡½æ•°
    perceptual_loss_fn = lpips.LPIPS(net='vgg').to(device)
    perceptual_loss_fn.eval()
    for param in perceptual_loss_fn.parameters():
        param.requires_grad = False
    
    # è®­ç»ƒå¾ªç¯
    caption_print_interval = config['training'].get('caption_print_interval', 200)
    
    print(f"\nğŸš€ Starting training from epoch {start_epoch}")
    print(f"   GAN training: {'Enabled' if model.use_gan else 'Disabled'}")
    
    for epoch in range(start_epoch, config['training']['num_epochs']):
        model.train()
        epoch_losses = []
        
        with tqdm(dataloader, desc=f"Epoch {epoch+1}/{config['training']['num_epochs']}") as pbar:
            for batch_idx, (images, captions) in enumerate(pbar):
                images = images.to(device)
                
                # å†³å®šæ˜¯å¦éœ€è¦è¿”å›tokens
                return_tokens = (global_step % caption_print_interval == 0)
                
                # GANè®­ç»ƒæ ‡å¿—
                train_gan = model.use_gan and d_optimizer and global_step > config['training'].get('gan_start_step', 5000)
                
                # === ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒåˆ¤åˆ«å™¨ï¼ˆå¦‚æœå¯ç”¨GANï¼‰ ===
                if train_gan and global_step % config['training']['d_steps_per_g'] == 0:
                    # ç”Ÿæˆå‡å›¾åƒç”¨äºåˆ¤åˆ«å™¨è®­ç»ƒ
                    with torch.no_grad():
                        fake_images, _ = model(images, return_tokens=False)
                        fake_images = fake_images.detach()
                    
                    # è®­ç»ƒåˆ¤åˆ«å™¨
                    d_optimizer.zero_grad()
                    
                    real_pred = model.discriminator(images)
                    fake_pred = model.discriminator(fake_images)
                    
                    d_loss = F.relu(1.0 - real_pred).mean() + F.relu(1.0 + fake_pred).mean()
                    d_loss.backward()
                    d_optimizer.step()
                
                # === ç¬¬äºŒæ­¥ï¼šè®­ç»ƒç”Ÿæˆå™¨ ===
                # å‰å‘ä¼ æ’­ï¼ˆç”¨äºé‡å»ºæŸå¤±ï¼‰
                reconstructed, info = model(images, return_tokens=return_tokens)
                
                # 1. é‡å»ºæŸå¤±
                recon_loss = F.mse_loss(reconstructed, images) * config['training']['loss_weights']['reconstruction']
                
                # 2. æ„ŸçŸ¥æŸå¤±
                with torch.no_grad():
                    perceptual_loss_value = perceptual_loss_fn(reconstructed, images).mean().item()
                perceptual_loss = perceptual_loss_value * config['training']['loss_weights']['perceptual']
                
                # 3. VQæŸå¤±
                vq_loss = sum(info['vq_losses'].values()) * config['training']['loss_weights']['vq']
                
                # 4. GANæŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                gan_loss_value = 0.0
                if train_gan:
                    # ä½¿ç”¨å½“å‰çš„reconstructedè®¡ç®—GANæŸå¤±
                    fake_pred_g = model.discriminator(reconstructed)
                    gan_loss = -fake_pred_g.mean() * config['training']['loss_weights']['gan']
                    gan_loss_value = gan_loss.item()
                    
                    # ç”Ÿæˆå™¨æ€»æŸå¤±ï¼ˆåŒ…å«GANæŸå¤±ï¼‰
                    g_loss = recon_loss + vq_loss + perceptual_loss + gan_loss
                else:
                    # ç”Ÿæˆå™¨æ€»æŸå¤±ï¼ˆä¸åŒ…å«GANæŸå¤±ï¼‰
                    g_loss = recon_loss + vq_loss + perceptual_loss
                
                # ç”Ÿæˆå™¨ä¼˜åŒ–
                g_optimizer.zero_grad(set_to_none=True)
                g_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(g_params, config['training']['gradient_clip'])
                
                g_optimizer.step()
                g_scheduler.step()
                
                # è®°å½•æŸå¤±
                total_loss = g_loss.item()
                epoch_losses.append(total_loss)
                
                # æ›´æ–°è¿›åº¦æ¡
                with torch.no_grad():
                    pbar.set_postfix({
                        'loss': f"{total_loss:.2f}",
                        'recon': f"{F.mse_loss(reconstructed, images).item():.3f}",
                        'gan': f"{gan_loss_value:.3f}",
                        'lr': f"{g_optimizer.param_groups[0]['lr']:.2e}"
                    })
                
                # æ‰“å°captionå¯¹æ¯”
                if return_tokens and info['vq_info']['global'].get('tokens'):
                    with torch.no_grad():
                        print(f"\n{'='*80}")
                        print(f"[Step {global_step}] Caption Comparison:")
                        print(f"{'='*80}")
                        
                        idx = np.random.randint(0, len(captions))
                        
                        print(f"\nğŸ“ Real Caption:")
                        print(f"   {captions[idx]}")
                        
                        tokens_dict = {}
                        for codebook_name in ['global', 'object', 'color', 'spatial', 'texture']:
                            if codebook_name in info['vq_info'] and info['vq_info'][codebook_name].get('tokens'):
                                tokens_dict[codebook_name] = [info['vq_info'][codebook_name]['tokens'][idx]]
                        
                        generated_caption = caption_generator.tokens_to_caption(tokens_dict)
                        print(f"\nğŸ¤– Generated Caption from Tokens:")
                        print(f"   {generated_caption}")
                        print(f"{'='*80}\n")
                
                # å®šæœŸä¿å­˜å’Œè¯„ä¼°
                if global_step % config['training']['sample_interval'] == 0:
                    with torch.no_grad():
                        # ç æœ¬ä½¿ç”¨ç‡
                        print(f"\nğŸ“ˆ Step {global_step} - Codebook Usage:")
                        for name, codebook in model.codebooks.items():
                            usage = codebook._get_usage_ratio()
                            perplexity = info['vq_info'][name]['perplexity'].item()
                            print(f"  {name}: {usage:.2%} (perplexity: {perplexity:.1f})")
                        
                        # ä¿å­˜æ ·æœ¬ - ä½¿ç”¨å›ºå®šçš„æµ‹è¯•å›¾åƒ
                        if fixed_test_images is not None:
                            # å¯¹å›ºå®šæµ‹è¯•å›¾åƒè¿›è¡Œé‡å»º
                            model.eval()
                            test_reconstructed, _ = model(fixed_test_images, return_tokens=False)
                            model.train()
                            
                            # ä¿å­˜å›ºå®šæµ‹è¯•å›¾åƒçš„é‡å»ºç»“æœ
                            save_samples(model, fixed_test_images, test_reconstructed, 
                                       os.path.join(config['output_dir'], f'fixed_samples_step_{global_step}.png'))
                            
                            # è®¡ç®—å›ºå®šæµ‹è¯•å›¾åƒçš„PSNRå’ŒSSIM
                            psnr = calculate_psnr(fixed_test_images, test_reconstructed)
                            ssim = calculate_ssim(fixed_test_images, test_reconstructed)
                            print(f"  ğŸ“Š Fixed Test Images - PSNR: {psnr:.2f}, SSIM: {ssim:.3f}")
                        else:
                            # å¦‚æœæ²¡æœ‰å›ºå®šæµ‹è¯•å›¾åƒï¼Œä½¿ç”¨å½“å‰batchçš„å›¾åƒï¼ˆåŸå§‹è¡Œä¸ºï¼‰
                            save_samples(model, images[:8], reconstructed[:8], 
                                       os.path.join(config['output_dir'], f'samples_step_{global_step}.png'))
                            
                            # è®¡ç®—PSNRå’ŒSSIM
                            psnr = calculate_psnr(images, reconstructed)
                            ssim = calculate_ssim(images, reconstructed)
                            print(f"  ğŸ“Š PSNR: {psnr:.2f}, SSIM: {ssim:.3f}\n")
                
                global_step += 1
                
                # å®šæœŸæ¸…ç†æ˜¾å­˜
                if global_step % 50 == 0:
                    torch.cuda.empty_cache()
                    gc.collect()
        
        # epochç»“æŸåçš„å¤„ç†
        avg_epoch_loss = np.mean(epoch_losses)
        print(f"\nğŸ“Š Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}")
        
        # æ›´æ–°æœ€ä½³æŸå¤±
        if avg_epoch_loss < best_loss:
            best_loss = avg_epoch_loss
            print(f"ğŸ‰ New best loss: {best_loss:.4f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % config['training']['checkpoint_interval'] == 0:
            torch.cuda.empty_cache()
            gc.collect()
            
            # æ”¶é›†ç æœ¬ç»Ÿè®¡ä¿¡æ¯
            codebook_stats = {}
            for name, codebook in model.codebooks.items():
                codebook_stats[name] = {
                    'usage_count': codebook.usage_count.cpu(),
                    'total_count': codebook.total_count.cpu()
                }
            
            checkpoint = {
                'epoch': epoch,
                'global_step': global_step,
                'best_loss': best_loss,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': {
                    'g_optimizer': g_optimizer.state_dict(),
                },
                'scheduler_state_dict': g_scheduler.state_dict(),
                'codebook_stats': codebook_stats,
                'config': config
            }
            
            if model.use_gan and d_optimizer:
                checkpoint['optimizer_state_dict']['d_optimizer'] = d_optimizer.state_dict()
            
            checkpoint_path = os.path.join(config['output_dir'], f'checkpoint_epoch_{epoch+1}.pt')
            torch.save(checkpoint, checkpoint_path)
            
            # åŒæ—¶ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_epoch_loss == best_loss:
                best_path = os.path.join(config['output_dir'], 'best_model.pt')
                torch.save(checkpoint, best_path)
                print(f"ğŸ’¾ Saved best model to {best_path}")
            
            del checkpoint
            torch.cuda.empty_cache()
            gc.collect()
            
            print(f"âœ… Saved checkpoint for epoch {epoch+1}")

# ================== ä¸»ç¨‹åº ==================
if __name__ == "__main__":
    # é…ç½®
    config = {
        'model': {
            'encoder': {
                'base_channels': 64
            },
            'decoder': {
                'base_channels': 64
            },
            'discriminator_channels': 32,
            'codebooks': {
                'global': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/global',
                    'embedding_dim': 768,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                },
                'object': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/object',
                    'embedding_dim': 512,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                },
                'color': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/color',
                    'embedding_dim': 256,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                },
                'spatial': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/spatial',
                    'embedding_dim': 384,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                },
                'texture': {
                    'path': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/codebook5/texture',
                    'embedding_dim': 256,
                    'commitment_cost': 0.25,
                    'decay': 0.99,
                    'epsilon': 1e-5
                }
            }
        },
        'data': {
            'train_dir': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/train/data',
            'image_size': 128,
            'num_workers': 4,
            'max_samples': None,
            # æ·»åŠ å›ºå®šæµ‹è¯•å›¾åƒè·¯å¾„
            'fixed_test_images': [
                '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/train/data/000000000072.jpg',
                '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/train/data/000000581118.jpg',
                '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/train/data/000000576981.jpg',
                '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/coco-2017/train/data/000000495079.jpg',
                # å¯ä»¥æ·»åŠ æ›´å¤šæµ‹è¯•å›¾åƒ
            ]
        },
        'training': {
            'batch_size': 16,
            'num_epochs': 200,
            'learning_rate': 0.0001,
            'use_gan': False,
            'gan_loss_type': 'standard',
            'gan_start_step': 5000,  # å¯ä»¥è®¾ç½®ä¸º0ç«‹å³å¼€å§‹GANè®­ç»ƒ
            'd_steps_per_g': 1,
            'gradient_clip': 1.0,
            'loss_weights': {
                'reconstruction': 1.0,
                'perceptual': 0.1,
                'vq': 1.0,
                'gan': 0.1
            },
            'sample_interval': 500,
            'checkpoint_interval': 5,
            'caption_print_interval': 200
        },
        'output_dir': '/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/VQ_GAN5/outputs/checkpoints_custom'
    }
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å‡å°‘æ˜¾å­˜ç¢ç‰‡
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
    
    # æŒ‡å®šè¦æ¢å¤çš„checkpointè·¯å¾„
    # resume_checkpoint = "/media/a/484ab06d-9814-4785-89b6-d05139c8bca2/a/PengYX/VQ-LLM/VQ_GAN5/outputs/checkpoint_epoch_45.pt"
    resume_checkpoint = None
    
    # å¼€å§‹è®­ç»ƒï¼ˆä»checkpointæ¢å¤ï¼‰
    train_fixed_semantic_vqgan_resume(config, resume_from=resume_checkpoint)